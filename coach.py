# file: coach.py (ABSOLUTE FINAL VERSION - Corrected Args)
import torch
import torch.nn.functional as F
import torch.optim as optim
import queue
import threading
import numpy as np
import tqdm
import random
import os
import re
import time
from torch.optim.lr_scheduler import CosineAnnealingLR

from neural_net import ExtendedConnectNet
from config import args
import cpp_mcts_engine

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def find_latest_model_file():
    path = "."
    max_epoch = 0
    latest_file = None
    pattern = re.compile(r"model_(\d+)\.pth")
    for f in os.listdir(path):
        match = pattern.match(f)
        if match:
            epoch = int(match.group(1))
            if epoch > max_epoch:
                max_epoch = epoch
                latest_file = f
    start_epoch = max_epoch + 1 if latest_file else 1
    return latest_file, start_epoch

def inference_server_func(model, device, job_q, result_q, stop_event, batch_size, board_size):
    model.eval()
    with torch.no_grad():
        while not stop_event.is_set():
            # ==================== ä»è¿™é‡Œå¼€å§‹æ›¿æ¢ ====================

            state_batch, id_batch = [], []

            # 1. é˜»å¡å¼ç­‰å¾…ï¼Œç›´åˆ°è·å–åˆ°æ‰¹æ¬¡çš„ç¬¬ä¸€ä¸ªä»»åŠ¡
            try:
                # å¯ä»¥è®¾ç½®ä¸€ä¸ªè¾ƒé•¿çš„è¶…æ—¶ï¼Œæ¯”å¦‚1ç§’ã€‚å¦‚æœ1ç§’éƒ½æ²¡æœ‰ä»»åŠ¡ï¼Œå¯èƒ½å°±çœŸçš„æ²¡äº‹å¹²äº†
                req_id, state = job_q.get(timeout=1.0)
                state_batch.append(state)
                id_batch.append(req_id)
            except queue.Empty:
                # å¦‚æœé•¿æ—¶é—´æ²¡æœ‰ä»»åŠ¡ï¼Œåˆ™ç»§ç»­å¤–å±‚å¾ªç¯
                continue

            # 2. ç¬¬ä¸€ä¸ªä»»åŠ¡å·²æ”¶åˆ°ï¼Œç°åœ¨å¿«é€Ÿå°†é˜Ÿåˆ—ä¸­â€œå·²ç»å­˜åœ¨â€çš„å…¶ä»–ä»»åŠ¡ä¹Ÿæ‰«è¿›æ‰¹æ¬¡
            #    ç›´åˆ°æ‰¹æ¬¡æ»¡ï¼Œæˆ–è€…é˜Ÿåˆ—å˜ç©º
            while len(id_batch) < batch_size:
                try:
                    # ä½¿ç”¨get_nowait()æˆ–get(block=False)ï¼Œå®ƒä¸ä¼šç­‰å¾…ï¼Œé˜Ÿåˆ—ä¸ºç©ºåˆ™ç«‹å³æŠ›å‡ºå¼‚å¸¸
                    req_id, state = job_q.get_nowait()
                    state_batch.append(state)
                    id_batch.append(req_id)
                except queue.Empty:
                    # é˜Ÿåˆ—å·²ç©ºï¼Œè¯´æ˜æˆ‘ä»¬å·²å°†æ‰€æœ‰ç§¯å‹çš„ä»»åŠ¡éƒ½æ”¶é›†äº†ï¼Œå¯ä»¥è·³å‡ºå¾ªç¯å»å¤„ç†æ‰¹æ¬¡
                    break

            # ==================== åˆ°è¿™é‡Œæ›¿æ¢ç»“æŸ ====================

            # åç»­çš„ä»£ç ä¿æŒä¸å˜
            # if not state_batch: continue ...
            # state_tensor = ...
            # log_policies, values = model(state_tensor) ...
            if not state_batch:
                continue

            state_tensor = torch.tensor(np.array(state_batch), device=device, dtype=torch.float32)
            state_tensor = state_tensor.view(-1, 6, board_size, board_size)

            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
                log_policies, values = model(state_tensor)

            policies = torch.exp(log_policies).cpu().numpy()
            values = values.squeeze(-1).cpu().numpy()

            for req_id, policy, value in zip(id_batch, policies, values):
                result_q.put((req_id, policy.tolist(), float(value)))

class Coach:
    def __init__(self, model, args):
        self.model = model
        self.args = args
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.args['learning_rate'], weight_decay=0.0001)
        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=self.args['num_epochs'])
        self.scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))
        self.training_data = []

    def train(self):
        self.model.train()
        if len(self.training_data) < self.args['batch_size']: return
        batch = random.sample(self.training_data, self.args['batch_size'])
        states, target_policies, target_values = zip(*batch)
        states = torch.tensor(np.array(states), dtype=torch.float32).to(device)
        target_policies = torch.tensor(np.array(target_policies), dtype=torch.float32).to(device)
        target_values = torch.tensor(np.array(target_values), dtype=torch.float32).unsqueeze(1).to(device)
        states = states.view(-1, 6, self.args['board_size'], self.args['board_size'])
        self.optimizer.zero_grad(set_to_none=True)
        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
            pred_log_policies, pred_values = self.model(states)
            policy_loss = -torch.sum(target_policies * pred_log_policies) / len(target_policies)
            value_loss = F.mse_loss(pred_values, target_values)
            total_loss = policy_loss + self.args['value_loss_weight'] * value_loss
        self.scaler.scale(total_loss).backward()
        self.scaler.step(self.optimizer)
        self.scaler.update()

    def learn(self, start_epoch=1):
        for i in range(start_epoch, start_epoch + self.args['num_iterations']):
            print(f"------ è¿­ä»£è½®æ¬¡: {i} ------")
            print("æ­¥éª¤1ï¼šå¯åŠ¨C++å¼•æ“è¿›è¡Œè‡ªæˆ‘å¯¹å¼ˆ (æ­¤è¿‡ç¨‹å°†é˜»å¡ï¼Œè¯·è€å¿ƒç­‰å¾…)...")
            job_queue = queue.Queue(maxsize=self.args['batch_size'] * 2)
            result_queue = queue.Queue()
            final_data_queue = queue.Queue()
            stop_event = threading.Event()
            server_thread = threading.Thread(
                target=inference_server_func,
                args=(self.model, device, job_queue, result_queue, stop_event, self.args['batch_size'], self.args['board_size'])
            )
            server_thread.start()

            # =================== å…³é”®ä¿®æ­£åœ¨è¿™é‡Œ ===================
            # åˆ›å»ºC++ä»£ç çœŸæ­£éœ€è¦çš„å‚æ•°å­—å…¸
            cpp_args = {
                'num_selfPlay_episodes': self.args['num_selfPlay_episodes'], # C++ç”¨è¿™ä¸ªæ¥å†³å®šæ€»ä»»åŠ¡æ•°
                'num_cpu_threads': self.args['num_cpu_threads'],             # C++ç”¨è¿™ä¸ªæ¥å†³å®šçº¿ç¨‹æ± å¤§å°
                'num_searches': self.args['num_searches']                    # C++ MCTSçš„æ¨¡æ‹Ÿæ¬¡æ•°
            }
            # ====================================================

            cpp_mcts_engine.run_parallel_self_play(job_queue, result_queue, final_data_queue, cpp_args)

            stop_event.set()
            server_thread.join()

            print("\nè‡ªæˆ‘å¯¹å¼ˆå®Œæˆï¼æ­£åœ¨æ”¶é›†æ•°æ®...")
            with tqdm.tqdm(total=self.args['num_selfPlay_episodes'], desc="æ”¶é›†æ•°æ®") as pbar:
                while pbar.n < self.args['num_selfPlay_episodes']:
                    try:
                        result = final_data_queue.get_nowait()
                        if result.get("type") == "data":
                            self.training_data.extend(result.get("data", []))
                            pbar.update(1)
                    except queue.Empty:
                        break

            print(f"\nç»éªŒåº“å¤§å°: {len(self.training_data)}")
            self.training_data = self.training_data[-self.args['data_max_size']:]

            print("\næ­¥éª¤2ï¼šè®­ç»ƒç¥ç»ç½‘ç»œ (ä½¿ç”¨GPU)...")
            if not self.training_data:
                print("ç»éªŒåº“ä¸ºç©ºï¼Œè·³è¿‡æœ¬è½®è®­ç»ƒã€‚")
            else:
                self.model.train()
                for _ in tqdm.tqdm(range(self.args['num_epochs']), desc="è®­ç»ƒæ¨¡å‹"):
                    self.train()

            self.scheduler.step()
            torch.save(self.model.state_dict(), f"model_{i}.pth")
            print(f"æ¨¡å‹ model_{i}.pth å·²ä¿å­˜ã€‚")
        print(f"\nè®­ç»ƒå®Œæˆï¼")

    def evaluate_models(self, model1_path, model2_path):
        print(f"\n------ å¼€å§‹è¯„ä¼° (C++ å¼•æ“é©±åŠ¨) ------")
        if not model1_path or not model2_path:
            print("ç¼ºå°‘æ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡è¯„ä¼°ã€‚")
            return
        device_eval = torch.device("cpu")
        try:
            model1 = ExtendedConnectNet(board_size=self.args['board_size'], num_res_blocks=self.args['num_res_blocks'], num_hidden=self.args['num_hidden']).to(device_eval)
            model1.load_state_dict(torch.load(model1_path, map_location=device_eval))
            model1.eval()
            model2 = ExtendedConnectNet(board_size=self.args['board_size'], num_res_blocks=self.args['num_res_blocks'], num_hidden=self.args['num_hidden']).to(device_eval)
            model2.load_state_dict(torch.load(model2_path, map_location=device_eval))
            model2.eval()
        except Exception as e:
            print(f"åŠ è½½è¯„ä¼°æ¨¡å‹å¤±è´¥: {e}, è·³è¿‡è¯„ä¼°ã€‚")
            return

        scores = {1: 0, -1: 0, 0: 0}
        eval_args = {
            'num_eval_simulations': self.args.get('num_eval_simulations', 20),
            'board_size': self.args['board_size']
        }
        for i in tqdm.tqdm(range(self.args['num_eval_games']), desc="è¯„ä¼°å¯¹æˆ˜"):
            p1 = model2 if i % 2 == 0 else model1
            p2 = model1 if i % 2 == 0 else model2
            winner = cpp_mcts_engine.play_game_for_eval(p1, p2, eval_args)
            if winner == 1: scores[1 if i % 2 == 0 else -1] += 1
            elif winner == -1: scores[-1 if i % 2 == 0 else 1] += 1
            else: scores[0] += 1

        win_rate = scores[1] / self.args['num_eval_games'] if self.args['num_eval_games'] > 0 else 0
        print("\n------ è¯„ä¼°ç»“æœ ------")
        print(f"æ€»å¯¹å±€æ•°: {self.args['num_eval_games']}")
        print(f"æ–°æ¨¡å‹ vs æ—§æ¨¡å‹ (èƒœ/è´Ÿ/å¹³): {scores[1]} / {scores[-1]} / {scores[0]}")
        print(f"æ–°æ¨¡å‹èƒœç‡: {win_rate:.2%}")
        if win_rate > self.args.get('eval_win_rate', 0.55):
            print("ç»“è®ºï¼šæ–°æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼ğŸ‘")
        else:
            print("ç»“è®ºï¼šæ–°æ¨¡å‹æå‡ä¸æ˜æ˜¾æˆ–æ²¡æœ‰æå‡ã€‚")

if __name__ == '__main__':
    print(f"å°†è¦ä½¿ç”¨çš„è®¾å¤‡ (ä¸»è¿›ç¨‹/è®­ç»ƒ): {device}")
    model_before_training, start_epoch = find_latest_model_file()
    model = ExtendedConnectNet(
        board_size=args['board_size'],
        num_res_blocks=args['num_res_blocks'],
        num_hidden=args['num_hidden']
    ).to(device)
    if model_before_training:
        try:
            print(f"æ‰¾åˆ°æœ€æ–°æ¨¡å‹ {model_before_training}ï¼Œå°†ä»ç¬¬ {start_epoch} è½®å¼€å§‹ç»§ç»­è®­ç»ƒ...")
            model.load_state_dict(torch.load(model_before_training, map_location=device))
            print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä»é›¶å¼€å§‹è®­ç»ƒã€‚")
            start_epoch = 1
            model_before_training = None
    else:
        print("æœªæ‰¾åˆ°ä»»ä½•å·²æœ‰æ¨¡å‹ï¼Œå°†ä»ç¬¬ 1 è½®å¼€å§‹å…¨æ–°è®­ç»ƒã€‚")
    coach = Coach(model, args)
    coach.learn(start_epoch=start_epoch)
    model_after_training, _ = find_latest_model_file()
    if model_before_training and model_after_training != model_before_training:
        coach.evaluate_models(model_before_training, model_after_training)
    else:
        print("\næœªè¿›è¡Œæœ‰æ•ˆçš„æ–°ä¸€è½®è®­ç»ƒæˆ–æœªæ‰¾åˆ°æ—§æ¨¡å‹ï¼Œè·³è¿‡è¯„ä¼°ã€‚")