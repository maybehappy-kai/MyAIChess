# file: coach.py (é›†æˆäº†æ¯è½®è¯„ä¼°ä¸æ»‘åŠ¨çª—å£ç»éªŒæ± çš„æœ€ç»ˆç‰ˆ)
import torch
import torch.nn.functional as F
import torch.optim as optim
import queue
import threading
import numpy as np
import tqdm
import random
import os
import re
from torch.optim.lr_scheduler import CosineAnnealingLR
from collections import deque
import platform
import ctypes

from neural_net import ExtendedConnectNet
from config import args
import cpp_mcts_engine


def get_augmented_data(state, policy, board_size, num_channels):
    """
    å¯¹å•ä¸ªè®­ç»ƒæ ·æœ¬è¿›è¡Œ8ç§å¯¹ç§°å˜æ¢çš„æ•°æ®å¢å¼ºã€‚
    æ­¤ç‰ˆæœ¬ç¡®ä¿æ‰€æœ‰å¢å¼ºæ•°æ®éƒ½æ˜¯ç‹¬ç«‹çš„å†…å­˜å‰¯æœ¬ã€‚
    """
    state_np = np.array(state).reshape(num_channels, board_size, board_size)
    policy_np = np.array(policy).reshape(board_size, board_size)
    augmented_data = []
    for i in range(4):
        rotated_state = np.rot90(state_np, i, axes=(1, 2))
        rotated_policy = np.rot90(policy_np, i)
        augmented_data.append((rotated_state.copy(), rotated_policy.flatten()))
        flipped_state = np.flip(rotated_state, axis=2)
        flipped_policy = np.flip(rotated_policy, axis=1)
        augmented_data.append((flipped_state.copy(), flipped_policy.flatten()))
    return augmented_data


def clear_windows_memory():
    if platform.system() == "Windows":
        try:
            ctypes.windll.psapi.EmptyWorkingSet(ctypes.windll.kernel32.GetCurrentProcess())
            print("[System] Windows memory working set has been cleared.")
        except Exception as e:
            print(f"[System] Failed to clear Windows memory working set: {e}")


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def transfer_weights(new_model, path_to_old_weights):
    """
    å°†æ—§æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯è¾ƒå°çš„æ¨¡å‹ï¼‰çš„æƒé‡åŠ è½½åˆ°æ–°æ¨¡å‹ä¸­ã€‚
    åªåŠ è½½å±‚åå’Œæƒé‡å½¢çŠ¶éƒ½åŒ¹é…çš„å±‚ã€‚
    """
    print(f"--- å¯åŠ¨è¿ç§»å­¦ä¹ ï¼Œä» '{path_to_old_weights}' åŠ è½½æƒé‡ ---")
    old_state_dict = torch.load(path_to_old_weights, map_location=torch.device('cpu'))
    new_model_state_dict = new_model.state_dict()
    loaded_count, skipped_count = 0, 0
    for name, param in old_state_dict.items():
        if name in new_model_state_dict and new_model_state_dict[name].shape == param.shape:
            new_model_state_dict[name].copy_(param)
            loaded_count += 1
        else:
            skipped_count += 1
    new_model.load_state_dict(new_model_state_dict)
    print(f"--- è¿ç§»å­¦ä¹ å®Œæˆã€‚æˆåŠŸè¿ç§» {loaded_count} ä¸ªå±‚ï¼Œè·³è¿‡ {skipped_count} ä¸ªä¸å…¼å®¹å±‚ã€‚ ---")
    return new_model


def save_model(model, epoch, args):
    """
    ä¿å­˜æ¨¡å‹ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆå¸¦ç»“æ„ä¿¡æ¯çš„æ–‡ä»¶å (åŒæ—¶ä¿å­˜ .pth å’Œ .pt)
    """
    num_channels = args['num_channels']
    base_filename = f"model_{epoch}_{args['num_res_blocks']}x{args['num_hidden']}_{num_channels}c"
    model_path_pth = f"{base_filename}.pth"
    model_path_pt = f"{base_filename}.pt"

    torch.save(model.state_dict(), model_path_pth)
    print(f"æ¨¡å‹ {model_path_pth} å·²ä¿å­˜ã€‚")

    model.eval()
    example_input = torch.rand(1, num_channels, args['board_size'], args['board_size']).to(device)
    try:
        traced_script_module = torch.jit.trace(model, example_input)
        traced_script_module.save(model_path_pt)
        print(f"TorchScriptæ¨¡å‹ {model_path_pt} å·²æˆåŠŸå¯¼å‡ºã€‚")
    except Exception as e:
        print(f"ã€é”™è¯¯ã€‘å¯¼å‡ºTorchScriptæ¨¡å‹å¤±è´¥: {e}")


def find_latest_model_file():
    """
    æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶ï¼Œå½“è½®æ¬¡ï¼ˆepochï¼‰ç›¸åŒæ—¶ï¼Œé€‰æ‹©æœ€è¿‘è¢«ä¿®æ”¹çš„æ–‡ä»¶ã€‚
    """
    path = "."
    max_epoch = -1
    latest_file_info = None
    latest_mtime = -1
    pattern = re.compile(r"model_(\d+)_(\d+)x(\d+)_(\d+)c\.pth")

    for f in os.listdir(path):
        match = pattern.match(f)
        if match:
            epoch = int(match.group(1))
            full_path = os.path.join(path, f)
            mtime = os.path.getmtime(full_path)
            if epoch > max_epoch or (epoch == max_epoch and mtime > latest_mtime):
                max_epoch = epoch
                latest_mtime = mtime
                latest_file_info = {
                    'path': f,
                    'epoch': epoch,
                    'res_blocks': int(match.group(2)),
                    'hidden_units': int(match.group(3)),
                    'channels': int(match.group(4))
                }
    return latest_file_info


class Coach:
    def __init__(self, model, args):
        self.model = model  # self.model å°†å§‹ç»ˆä»£è¡¨â€œå½“å‰æœ€ä¼˜æ¨¡å‹â€
        self.args = args
        self.training_data = deque(maxlen=self.args['data_max_size'])
        self.scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))

    # ====================== ã€æ ¸å¿ƒæ”¹åŠ¨ 1ã€‘trainå‡½æ•°å‚æ•°åŒ– ======================
    # è®©trainå‡½æ•°å¯ä»¥çµæ´»åœ°è®­ç»ƒä»»ä½•æ¨¡å‹ï¼Œè€Œä¸ä»…ä»…æ˜¯self.model
    # è¿™æ˜¯ä¿®æ”¹åçš„ train å‡½æ•°
    def train(self, model_to_train, optimizer, scheduler=None):
        """
        å¯¹ç»™å®šçš„æ¨¡å‹å’Œä¼˜åŒ–å™¨æ‰§è¡Œä¸€ä¸ªè®­ç»ƒå‘¨æœŸï¼Œå¹¶è¿”å›å¹³å‡æŸå¤±ã€‚
        """
        model_to_train.train()

        # --- æ–°å¢ï¼šåˆå§‹åŒ–æŸå¤±åˆ—è¡¨ ---
        policy_losses = []
        value_losses = []

        for _ in tqdm.tqdm(range(self.args.get('training_steps_per_iteration', 500)), desc="è®­ç»ƒæ¨¡å‹ Steps"):
            if len(self.training_data) < self.args['batch_size']:
                # ...
                continue

            batch = random.sample(self.training_data, self.args['batch_size'])
            # ... (æ•°æ®å¢å¼ºé€»è¾‘ä¸å˜) ...
            augmented_batch = []
            for state, policy, value in batch:
                augmented_samples = get_augmented_data(state, policy, self.args['board_size'],
                                                       self.args['num_channels'])
                for aug_s, aug_p in augmented_samples:
                    augmented_batch.append((aug_s, aug_p, value))

            states, target_policies, target_values = zip(*augmented_batch)
            states = torch.tensor(np.array(states), dtype=torch.float32).to(device)
            target_policies = torch.tensor(np.array(target_policies), dtype=torch.float32).to(device)
            target_values = torch.tensor(np.array(target_values), dtype=torch.float32).unsqueeze(1).to(device)

            optimizer.zero_grad(set_to_none=True)
            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
                pred_log_policies, pred_values = model_to_train(states)
                policy_loss = -torch.sum(target_policies * pred_log_policies) / len(target_policies)
                value_loss = F.mse_loss(pred_values, target_values)
                total_loss = policy_loss + self.args['value_loss_weight'] * value_loss

            # --- æ–°å¢ï¼šå°†å½“å‰æ‰¹æ¬¡çš„æŸå¤±è®°å½•ä¸‹æ¥ ---
            policy_losses.append(policy_loss.item())
            value_losses.append(value_loss.item())

            self.scaler.scale(total_loss).backward()
            self.scaler.step(optimizer)
            self.scaler.update()

            if scheduler is not None:
                scheduler.step()

        # --- æ–°å¢ï¼šè®¡ç®—å¹¶è¿”å›å¹³å‡æŸå¤± ---
        avg_policy_loss = np.mean(policy_losses) if policy_losses else 0
        avg_value_loss = np.mean(value_losses) if value_losses else 0
        return avg_policy_loss, avg_value_loss

    # ====================== ã€æ ¸å¿ƒæ”¹åŠ¨ 2ã€‘learnå‡½æ•°æµç¨‹é‡æ„ ======================
    def learn(self, start_epoch=1):
        """
        æ‰§è¡ŒåŒ…å«â€œè‡ªå¯¹å¼ˆ->è®­ç»ƒ->è¯„ä¼°->æ™‹å‡/ä¸¢å¼ƒâ€å¾ªç¯çš„å®Œæ•´å­¦ä¹ è¿‡ç¨‹ã€‚
        """
        # åˆå§‹æ—¶ï¼Œself.model å°±æ˜¯æœ€ä¼˜æ¨¡å‹
        best_model_info = find_latest_model_file()
        if not best_model_info:
            print("ã€ä¸¥é‡é”™è¯¯ã€‘æ— æ³•æ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶ï¼ç¨‹åºé€€å‡ºã€‚")
            return

        model_was_promoted = True
        elo_best_model = 1500
        for i in range(start_epoch, start_epoch + self.args['num_iterations']):
            print(f"\n{'=' * 20} è¿­ä»£è½®æ¬¡: {i} {'=' * 20}")

            # --- æ­¥éª¤ 1: ä½¿ç”¨å½“å‰æœ€ä¼˜æ¨¡å‹è¿›è¡Œè‡ªå¯¹å¼ˆ ---
            # åœ¨ learn æ–¹æ³•å†…éƒ¨...
            best_model_path_pt = best_model_info['path'].replace('.pth', '.pt')

            # --- åŠ¨æ€è°ƒæ•´é€»è¾‘ ---
            cpp_args = self.args.copy()  # åˆ›å»ºä¸€ä¸ªä¸´æ—¶å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹é…ç½®
            if model_was_promoted:
                print(f"æ­¥éª¤1: æ¨¡å‹åˆšæ™‹å‡ï¼Œæ‰§è¡Œä¸€è½®å®Œæ•´çš„è‡ªå¯¹å¼ˆ ({cpp_args['num_selfPlay_episodes']} å±€)...")
                # ä½¿ç”¨å®Œæ•´çš„å±€æ•°ï¼Œæ— éœ€ä¿®æ”¹ cpp_args
            else:
                # æ¨¡å‹æœªæ™‹å‡ï¼Œåªæ‰§è¡Œå°‘é‡å¯¹å¼ˆæ¥åˆ·æ–°æ•°æ®
                small_episodes = max(1, int(cpp_args['num_selfPlay_episodes'] * 0.1))
                cpp_args['num_selfPlay_episodes'] = small_episodes
                print(f"æ­¥éª¤1: æ¨¡å‹æœªæ™‹å‡ï¼Œæ‰§è¡Œä¸€è½®å°è§„æ¨¡å¢é‡è‡ªå¯¹å¼ˆ ({small_episodes} å±€)...")

            print(f"   ä½¿ç”¨æ¨¡å‹: '{best_model_path_pt}'")

            final_data_queue = queue.Queue()
            cpp_mcts_engine.run_parallel_self_play(
                best_model_path_pt,
                device.type == 'cuda',
                final_data_queue,
                cpp_args  # <-- æ³¨æ„è¿™é‡Œä½¿ç”¨çš„æ˜¯ä¿®æ”¹åçš„ cpp_args
            )

            # --- æ­¥éª¤ 2: æ”¶é›†æ•°æ®å¹¶æ”¾å…¥â€œæ»‘åŠ¨çª—å£â€ç»éªŒæ±  ---
            # è¿™æ˜¯å…³é”®ï¼šæˆ‘ä»¬åªæ·»åŠ æ–°æ•°æ®ï¼Œæ—§æ•°æ®ä¼šå› dequeçš„maxlenè€Œè‡ªåŠ¨æ·˜æ±°
            print("è‡ªå¯¹å¼ˆå®Œæˆï¼æ­£åœ¨æ”¶é›†å’Œç­›é€‰æ–°æ•°æ®...")
            games_processed, good_steps_collected, bad_steps_discarded = 0, 0, 0
            policy_entropies = []
            while not final_data_queue.empty():
                try:
                    result = final_data_queue.get_nowait()
                    if result.get("type") == "data":
                        games_processed += 1
                        game_data = result.get("data", [])
                        enable_filtering = self.args.get('filter_zero_policy_data', True)
                        for state, policy, value in game_data:
                            if not enable_filtering or np.any(policy):
                                self.training_data.append((state, policy, value))
                                # --- æ–°å¢ï¼šè®¡ç®—å¹¶è®°å½•ç­–ç•¥ç†µ ---
                                p_vec = np.array(policy)
                                p_vec = p_vec[p_vec > 0]  # è¿‡æ»¤æ‰0æ¦‚ç‡ï¼Œé¿å…log(0)
                                if p_vec.size > 0:
                                    entropy = -np.sum(p_vec * np.log2(p_vec))
                                    policy_entropies.append(entropy)
                                good_steps_collected += 1
                            else:
                                bad_steps_discarded += 1
                except queue.Empty:
                    break

            print(
                f"æ•°æ®å¤„ç†å®Œæˆï¼æœ¬è½®å…±å¤„ç† {games_processed} å±€, æ”¶é›†åˆ° {good_steps_collected} ä¸ªæœ‰æ•ˆæ­¥éª¤, ä¸¢å¼ƒ {bad_steps_discarded} ä¸ªã€‚")
            print(f"å½“å‰æ€»ç»éªŒåº“å¤§å°: {len(self.training_data)}")

            # åœ¨ learn æ–¹æ³•çš„ä¸»å¾ªç¯ä¸­...
            # ... (æ­¥éª¤1å’Œæ­¥éª¤2çš„ä»£ç ) ...

            if len(self.training_data) < self.args['batch_size']:
                print("è­¦å‘Šï¼šç»éªŒæ± æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æœ¬è½®è®­ç»ƒå’Œè¯„ä¼°ã€‚")
                model_was_promoted = False  # ç¡®ä¿ä¸‹ä¸€è½®æ˜¯å°è§„æ¨¡è‡ªå¯¹å¼ˆ
                continue

            # --- æ–°å¢ï¼šä¸ºå¤šå€™é€‰å¾ªç¯è®¾ç½®ä¸€ä¸ªæ ‡å¿—ä½ ---
            promotion_achieved_this_iteration = False

            # --- æ–°å¢ï¼šåŒ…è£¹â€œè®­ç»ƒ-è¯„ä¼°-æ™‹å‡â€æµç¨‹çš„å†…å¾ªç¯ ---
            for candidate_idx in range(self.args.get('num_candidates_to_train', 1)):
                print(
                    f"\n{'--' * 15} æ­£åœ¨å°è¯•ç¬¬ {candidate_idx + 1} / {self.args.get('num_candidates_to_train', 1)} ä¸ªå€™é€‰æ¨¡å‹ {'--' * 15}")

                # --- æ­¥éª¤ 3: è®­ç»ƒå€™é€‰æ¨¡å‹ ---
                # (è¿™éƒ¨åˆ†ä»£ç é€»è¾‘ä¸å˜)
                print("\næ­¥éª¤3.1: è®­ç»ƒå€™é€‰æ¨¡å‹...")
                candidate_model = ExtendedConnectNet(
                    board_size=self.args['board_size'], num_res_blocks=self.args['num_res_blocks'],
                    num_hidden=self.args['num_hidden'], num_channels=self.args['num_channels']
                ).to(device)
                candidate_model.load_state_dict(self.model.state_dict())

                optimizer = optim.Adam(candidate_model.parameters(), lr=self.args['learning_rate'], weight_decay=0.0001)
                # --- ä¿®æ”¹ self.train çš„è°ƒç”¨æ–¹å¼ï¼Œä»¥æ¥æ”¶è¿”å›å€¼ ---
                avg_p_loss, avg_v_loss = self.train(candidate_model, optimizer)

                # --- æ–°å¢ï¼šç«‹å³æ‰“å°æœ¬æ¬¡è®­ç»ƒçš„æŸå¤± ---
                print(f"  - è®­ç»ƒæŸå¤±: Policy Loss={avg_p_loss:.4f}, Value Loss={avg_v_loss:.4f}")

                # --- æ­¥éª¤ 4: è¯„ä¼°å€™é€‰æ¨¡å‹ vs. æœ€ä¼˜æ¨¡å‹ ---
                # (è¿™éƒ¨åˆ†ä»£ç é€»è¾‘ä¸å˜)
                print("\næ­¥éª¤3.2: è¯„ä¼°å€™é€‰æ¨¡å‹ vs. æœ€ä¼˜æ¨¡å‹...")
                candidate_model_path_pt = f"candidate_{candidate_idx}.pt"  # ä½¿ç”¨ç´¢å¼•é¿å…æ–‡ä»¶åå†²çª
                candidate_model.eval()
                example_input = torch.rand(1, self.args['num_channels'], self.args['board_size'],
                                           self.args['board_size']).to(device)
                traced_script_module = torch.jit.trace(candidate_model, example_input)
                traced_script_module.save(candidate_model_path_pt)

                eval_args = self.args.copy()
                eval_args['num_eval_games'] = self.args.get('num_eval_games', 20) // 2
                eval_args['num_eval_simulations'] = self.args['num_searches']

                results = cpp_mcts_engine.run_parallel_evaluation(
                    best_model_path_pt,
                    candidate_model_path_pt,
                    device.type == 'cuda',
                    eval_args,
                    mode=0
                )

                model1_wins = results.get("model1_wins", 0)
                model2_wins = results.get("model2_wins", 0)
                draws = results.get("draws", 0)
                total_games = model1_wins + model2_wins + draws
                win_rate = model2_wins / total_games if total_games > 0 else 0

                print(f"è¯„ä¼°ç»“æœ: æ–° vs. æ—§ | èƒœ/è´Ÿ/å¹³: {model2_wins} / {model1_wins} / {draws}")
                print(f"æ–°æ¨¡å‹èƒœç‡: {win_rate:.2%}")

                # ... ç´§è·Ÿåœ¨æ‰“å°èƒœç‡ä¹‹å ...

                # --- æ–°å¢ï¼šEloè®¡ç®— ---
                elo_candidate = elo_best_model  # å€™é€‰æ¨¡å‹æŒ‘æˆ˜æ—¶ï¼Œé»˜è®¤å…¶Eloä¸å½“å‰æœ€ä¼˜æ¨¡å‹ç›¸åŒ

                # 1. è®¡ç®—æœŸæœ›èƒœç‡
                expected_win_rate_candidate = 1 / (1 + 10 ** ((elo_best_model - elo_candidate) / 400))
                expected_win_rate_best = 1 - expected_win_rate_candidate

                # 2. è®¡ç®—å®é™…å¾—åˆ†ï¼ˆèµ¢=1, å¹³=0.5, è¾“=0ï¼‰
                actual_score_candidate = model2_wins + 0.5 * draws
                actual_score_best = model1_wins + 0.5 * draws

                # 3. è®¡ç®—æœŸæœ›å¾—åˆ†
                expected_score_candidate = expected_win_rate_candidate * total_games
                expected_score_best = expected_win_rate_best * total_games

                # 4. æ›´æ–°Elo
                k_factor = self.args.get('elo_k_factor', 32)
                new_elo_candidate = elo_candidate + k_factor * (actual_score_candidate - expected_score_candidate)
                new_elo_best = elo_best_model + k_factor * (actual_score_best - expected_score_best)

                # æ‰“å°Eloå˜åŒ–
                elo_change_candidate = new_elo_candidate - elo_candidate
                print(f"  - Elo è¯„çº§: BestNet ({elo_best_model:.0f}) vs Candidate ({elo_candidate:.0f})")
                print(f"  - Elo å˜åŒ–: Candidate Elo -> {new_elo_candidate:.0f} ({elo_change_candidate:+.0f})")

                # æ€»æ˜¯åˆ é™¤ä¸´æ—¶çš„å€™é€‰æ¨¡å‹æ–‡ä»¶
                # ...

                # æ€»æ˜¯åˆ é™¤ä¸´æ—¶çš„å€™é€‰æ¨¡å‹æ–‡ä»¶
                if os.path.exists(candidate_model_path_pt):
                    os.remove(candidate_model_path_pt)

                # --- æ­¥éª¤ 5: ä¼˜èƒœåŠ£æ±° (ä¿®æ”¹åçš„é€»è¾‘) ---
                if win_rate > 0.55:
                    print(f"ã€æ¨¡å‹æ™‹å‡ã€‘å€™é€‰ {candidate_idx + 1} èƒœç‡è¾¾æ ‡ï¼Œå°†å…¶ä¿å­˜ä¸º model_{i} å¹¶è®¾ä¸ºæ–°çš„æœ€ä¼˜æ¨¡å‹ã€‚")
                    promotion_achieved_this_iteration = True
                    elo_best_model = new_elo_candidate  # <-- æ–°å¢æ­¤è¡Œï¼Œä¼ é€’Elo
                    self.model.load_state_dict(candidate_model.state_dict())
                    save_model(self.model, i, self.args)
                    best_model_info = find_latest_model_file()
                    br
            else:
                    print(f"ã€æ¨¡å‹ä¸¢å¼ƒã€‘å€™é€‰ {candidate_idx + 1} èƒœç‡æœªè¾¾æ ‡ã€‚")

            # --- å¾ªç¯ç»“æŸåï¼Œæ ¹æ®æœ¬è½®æ˜¯å¦å‘ç”Ÿæ™‹å‡æ¥æ›´æ–°ä¸‹ä¸€è½®çš„è‡ªå¯¹å¼ˆæ ‡å¿—ä½ ---
            model_was_promoted = promotion_achieved_this_iteration

            print(f"\n{'=' * 20} è¿­ä»£è½®æ¬¡: {i} æ€»ç»“ {'=' * 20}")
            print("æ€§èƒ½æŒ‡æ ‡:")
            print(f"  - å½“å‰æœ€ä¼˜æ¨¡å‹ Elo: {elo_best_model:.0f}")
            print("è¡Œä¸ºç»Ÿè®¡ (æ¥è‡ªæœ¬è½®è‡ªå¯¹å¼ˆ):")
            avg_entropy = np.mean(policy_entropies) if policy_entropies else 0
            print(f"  - å¹³å‡MCTSç­–ç•¥ç†µ: {avg_entropy:.3f} bits")
            print(f"{'=' * 56}")

            clear_windows_memory()

        print(f"\nå…¨éƒ¨ {self.args['num_iterations']} è½®è¿­ä»£å®Œæˆï¼")

    # è¿™ä¸ªå‡½æ•°ä¿ç•™ï¼Œç”¨äºæœ€ç»ˆçš„ã€æ›´è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Šï¼Œæˆ–è€…å¯ä»¥è¢«æ‰‹åŠ¨è°ƒç”¨
    def evaluate_models(self, model1_info, model2_info):
        # ... æ­¤å‡½æ•°å†…å®¹ä¿æŒä¸å˜ ...
        print(f"\n------ å¼€å§‹åˆ†ç»„è¯Šæ–­å¼è¯„ä¼° (C++ å¼•æ“é©±åŠ¨) ------")
        if not model1_info or not model2_info:
            print("è¯„ä¼°ç¼ºå°‘å¿…è¦çš„æ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡è¯„ä¼°ã€‚")
            return

        model1_pt_path = model1_info['path'].replace('.pth', '.pt')
        model2_pt_path = model2_info['path'].replace('.pth', '.pt')

        if not os.path.exists(model1_pt_path) or not os.path.exists(model2_pt_path):
            print("è¯„ä¼°ç¼ºå°‘å¿…è¦çš„.ptæ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡è¯„ä¼°ã€‚")
            return

        print(f"è¯„ä¼°æ¨¡å‹ (æ—§): {model1_pt_path}")
        print(f"è¯„ä¼°æ¨¡å‹ (æ–°): {model2_pt_path}")

        use_gpu = (device.type == 'cuda')
        total_games = self.args.get('num_eval_games', 100)
        games_per_side = total_games // 2
        if games_per_side == 0:
            print("è¯„ä¼°å±€æ•°è¿‡å°‘ï¼Œæ— æ³•è¿›è¡Œåˆ†ç»„è¯„ä¼°ã€‚")
            return

        eval_args = {
            'num_eval_games': games_per_side,
            'num_eval_simulations': self.args['num_searches'],
            'num_cpu_threads': self.args.get('num_cpu_threads', 18),
            'C': self.args['C'],
            'mcts_batch_size': self.args['mcts_batch_size'],
            'board_size': self.args['board_size'],
            'num_rounds': self.args['num_rounds'],
            'history_steps': self.args['history_steps'],
            'num_channels': self.args['num_channels'],
            'enable_territory_heuristic': self.args.get('enable_territory_heuristic', False),
            'territory_heuristic_weight': self.args.get('territory_heuristic_weight', 0.0),
            'enable_territory_penalty': self.args.get('enable_territory_penalty', False),
            'territory_penalty_strength': self.args.get('territory_penalty_strength', 0.0),
            'enable_ineffective_connection_penalty': self.args.get('enable_ineffective_connection_penalty', False),
            'ineffective_connection_penalty_factor': self.args.get('ineffective_connection_penalty_factor', 0.1),
        }

        print(f"\n[å®éªŒä¸€] æ–°æ¨¡å‹æ‰§é»‘ï¼Œè¿›è¡Œ {games_per_side} å±€...")
        results1 = cpp_mcts_engine.run_parallel_evaluation(
            model1_pt_path, model2_pt_path, use_gpu, eval_args, mode=2
        )
        new_as_p1_wins = results1.get("model2_wins", 0)
        old_as_p2_wins = results1.get("model1_wins", 0)
        draws1 = results1.get("draws", 0)

        print(f"\n[å®éªŒäºŒ] æ—§æ¨¡å‹æ‰§é»‘ï¼Œè¿›è¡Œ {games_per_side} å±€...")
        results2 = cpp_mcts_engine.run_parallel_evaluation(
            model1_pt_path, model2_pt_path, use_gpu, eval_args, mode=1
        )
        old_as_p1_wins = results2.get("model1_wins", 0)
        new_as_p2_wins = results2.get("model2_wins", 0)
        draws2 = results2.get("draws", 0)

        total_new_wins = new_as_p1_wins + new_as_p2_wins
        total_old_wins = old_as_p1_wins + old_as_p2_wins
        total_draws = draws1 + draws2

        print("\n------ è¯Šæ–­è¯„ä¼°ç»“æœ ------")
        print(f"æ–°æ¨¡å‹æ‰§å…ˆæ‰‹æ—¶ï¼Œæˆ˜ç»© (æ–° vs æ—§ | èƒœ/è´Ÿ/å¹³): {new_as_p1_wins} / {old_as_p2_wins} / {draws1}")
        print(f"æ—§æ¨¡å‹æ‰§å…ˆæ‰‹æ—¶ï¼Œæˆ˜ç»© (æ—§ vs æ–° | èƒœ/è´Ÿ/å¹³): {old_as_p1_wins} / {new_as_p2_wins} / {draws2}")
        print("---------------------------------")

        overall_win_rate = total_new_wins / (total_games) if total_games > 0 else 0
        print(f"ç»¼åˆæˆ˜ç»© - æ–° vs æ—§ (èƒœ/è´Ÿ/å¹³): {total_new_wins} / {total_old_wins} / {total_draws}")
        print(f"æ–°æ¨¡å‹ç»¼åˆèƒœç‡: {overall_win_rate:.2%}")

        if games_per_side > 0 and (new_as_p1_wins / games_per_side) > 0.9 and (old_as_p1_wins / games_per_side) > 0.9:
            print("\nã€è¯Šæ–­ç»“è®ºã€‘: AIå·²å‘ç°å¹¶æŒæ¡äº† 'å…ˆæ‰‹å¿…èƒœ' ç­–ç•¥ã€‚")
        elif overall_win_rate > self.args.get('eval_win_rate', 0.52):
            print("\nã€è¯Šæ–­ç»“è®ºã€‘: æ–°æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼ğŸ‘")
        else:
            print("\nã€è¯Šæ–­ç»“è®ºã€‘: æ–°æ¨¡å‹æå‡ä¸æ˜æ˜¾æˆ–æ²¡æœ‰æå‡ã€‚")


if __name__ == '__main__':
    # ====================== ä¸»å‡½æ•°é€»è¾‘ä¿æŒä¸å˜ ======================
    history_channels = (args.get('history_steps', 0) + 1) * 4
    meta_channels = 4
    total_channels = history_channels + meta_channels
    args['num_channels'] = total_channels

    print("=" * 50)
    print("MyAIChess é…ç½®åŠ è½½å®Œæˆ")
    print(f"å†å²æ­¥æ•°: {args.get('history_steps', 0)}")
    print(f"è®¡ç®—å‡ºçš„æ€»è¾“å…¥é€šé“æ•°: {args['num_channels']}")
    print("=" * 50)

    print(f"å°†è¦ä½¿ç”¨çš„è®¾å¤‡ (ä¸»è¿›ç¨‹/è®­ç»ƒ): {device}")

    latest_model_info = find_latest_model_file()
    start_epoch = 1

    current_model = ExtendedConnectNet(
        board_size=args['board_size'],
        num_res_blocks=args['num_res_blocks'],
        num_hidden=args['num_hidden'],
        num_channels=args['num_channels']
    ).to(device)

    # --- æ¨¡å‹åŠ è½½å’Œè¿ç§»å­¦ä¹ é€»è¾‘ä¿æŒä¸å˜ ---
    if latest_model_info is None:
        print("æœªæ‰¾åˆ°ä»»ä½•å·²æœ‰æ¨¡å‹ï¼Œå°†ä»ç¬¬ 1 è½®å¼€å§‹å…¨æ–°è®­ç»ƒã€‚")
        start_epoch = 1
        print("æ­£åœ¨åˆ›å»ºå¹¶ä¿å­˜åˆå§‹éšæœºæ¨¡å‹ (model_0)...")
        save_model(current_model, 0, args)
    else:
        print(f"æ‰¾åˆ°æœ€æ–°æ¨¡å‹: {latest_model_info['path']} (ç¬¬ {latest_model_info['epoch']} è½®)")
        start_epoch = latest_model_info['epoch'] + 1
        config_blocks = args['num_res_blocks']
        config_hidden = args['num_hidden']
        config_channels = args['num_channels']
        is_same_architecture = (latest_model_info['res_blocks'] == config_blocks and
                                latest_model_info['hidden_units'] == config_hidden and
                                latest_model_info['channels'] == config_channels)

        if is_same_architecture:
            print("æ¨¡å‹ç»“æ„ä¸å½“å‰é…ç½®ä¸€è‡´ï¼Œç›´æ¥åŠ è½½æƒé‡ç»§ç»­è®­ç»ƒã€‚")
            try:
                current_model.load_state_dict(torch.load(latest_model_info['path'], map_location=device))
                print("æƒé‡åŠ è½½æˆåŠŸï¼")
            except Exception as e:
                print(f"åŠ è½½æƒé‡å¤±è´¥: {e}ï¼Œå°†ä»éšæœºæƒé‡å¼€å§‹ã€‚")
                start_epoch = 1
        else:
            print("æ¨¡å‹ç»“æ„ä¸å½“å‰é…ç½®ä¸ä¸€è‡´ï¼Œå°†æ‰§è¡Œè‡ªåŠ¨è¿ç§»å­¦ä¹ ã€‚")
            print(
                f"  æ—§ç»“æ„: {latest_model_info['res_blocks']} res_blocks, {latest_model_info['hidden_units']} hidden, {latest_model_info.get('channels', 'N/A')} channels")
            print(f"  æ–°ç»“æ„: {config_blocks} res_blocks, {config_hidden} hidden, {config_channels} channels")
            try:
                current_model = transfer_weights(current_model, latest_model_info['path'])
                print("ä¸ºè¿ç§»å­¦ä¹ åçš„æ–°æ¨¡å‹åˆ›å»ºåŒ¹é…çš„ .pt æ–‡ä»¶...")
                save_model(current_model, latest_model_info['epoch'], args)
            except Exception as e:
                print(f"è¿ç§»å­¦ä¹ å¤±è´¥: {e}ï¼Œå°†ä»éšæœºæƒé‡å¼€å§‹è®­ç»ƒæ–°ç»“æ„æ¨¡å‹ã€‚")

    # --- å¯åŠ¨è®­ç»ƒ ---
    coach = Coach(current_model, args)
    coach.learn(start_epoch=start_epoch)

    print("\nè®­ç»ƒå…¨éƒ¨å®Œæˆï¼Œæ­£åœ¨æ‰‹åŠ¨æ¸…ç†å†…å­˜...")
    if 'coach' in locals() and hasattr(coach, 'training_data'):
        coach.training_data.clear()
        print("ç»éªŒå›æ”¾æ± å·²æ¸…ç©ºã€‚")
    import gc

    gc.collect()
    print("å†…å­˜æ¸…ç†å®Œæˆã€‚ç¨‹åºå³å°†é€€å‡ºã€‚")