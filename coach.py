# file: coach.py (å·²ä¿®æ­£ NameError çš„æœ€ç»ˆå®Œæ•´ç‰ˆ)
import torch
import torch.nn.functional as F
import torch.optim as optim
import queue
import threading
import numpy as np
import tqdm
import random
import os
import re
from torch.optim.lr_scheduler import CosineAnnealingLR
from collections import deque, Counter
import platform
import ctypes
import pickle

from neural_net import ExtendedConnectNet
from config import args
import cpp_mcts_engine


def get_augmented_data(state, policy, board_size, num_channels):
    state_np = np.array(state).reshape(num_channels, board_size, board_size)
    policy_np = np.array(policy).reshape(board_size, board_size)
    augmented_data = []
    for i in range(4):
        rotated_state = np.rot90(state_np, i, axes=(1, 2))
        rotated_policy = np.rot90(policy_np, i)
        augmented_data.append((rotated_state.copy(), rotated_policy.flatten()))
        flipped_state = np.flip(rotated_state, axis=2)
        flipped_policy = np.flip(rotated_policy, axis=1)
        augmented_data.append((flipped_state.copy(), flipped_policy.flatten()))
    return augmented_data


def get_move_number_from_state(state, args):
    """ä»çŠ¶æ€å‘é‡ä¸­è§£æå‡ºå½“å‰æ˜¯ç¬¬å‡ æ­¥"""
    try:
        num_channels = args['num_channels']
        board_size = args['board_size']
        max_total_moves = args['num_rounds'] * 2
        history_steps = args.get('history_steps', 0)

        # æ¸¸æˆè¿›åº¦åœ¨å…ƒæ•°æ®é€šé“çš„ç¬¬2ä¸ªå¹³é¢ (ç´¢å¼•ä¸º1)
        progress_channel_idx = (history_steps + 1) * 4 + 1

        state_np = np.array(state).reshape(num_channels, board_size, board_size)

        # ä»è¯¥å¹³é¢ä»»æ„ä½ç½®è·å–è¿›åº¦å€¼
        progress = state_np[progress_channel_idx, 0, 0]

        # åå½’ä¸€åŒ–å¾—åˆ°æ­¥æ•°
        move_number = round(progress * max_total_moves)
        return int(move_number)
    except Exception as e:
        print(f"[è­¦å‘Š] ä»çŠ¶æ€è§£ææ­¥æ•°å¤±è´¥: {e}")
        return -1 # è¿”å›ä¸€ä¸ªé”™è¯¯æ ‡è¯†


def clear_windows_memory():
    if platform.system() == "Windows":
        try:
            ctypes.windll.psapi.EmptyWorkingSet(ctypes.windll.kernel32.GetCurrentProcess())
            print("[System] Windows memory working set has been cleared.")
        except Exception as e:
            print(f"[System] Failed to clear Windows memory working set: {e}")


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def transfer_weights(new_model, path_to_old_weights):
    print(f"--- å¯åŠ¨è¿ç§»å­¦ä¹ ï¼Œä» '{path_to_old_weights}' åŠ è½½æƒé‡ ---")
    old_state_dict = torch.load(path_to_old_weights, map_location=torch.device('cpu'))
    new_model_state_dict = new_model.state_dict()
    loaded_count, skipped_count = 0, 0
    for name, param in old_state_dict.items():
        if name in new_model_state_dict and new_model_state_dict[name].shape == param.shape:
            new_model_state_dict[name].copy_(param)
            loaded_count += 1
        else:
            skipped_count += 1
    new_model.load_state_dict(new_model_state_dict)
    print(f"--- è¿ç§»å­¦ä¹ å®Œæˆã€‚æˆåŠŸè¿ç§» {loaded_count} ä¸ªå±‚ï¼Œè·³è¿‡ {skipped_count} ä¸ªä¸å…¼å®¹å±‚ã€‚ ---")
    return new_model


# --- åœ¨ coach.py ä¸­æ·»åŠ æ­¤å‡½æ•° ---
def remove_old_models(args, keep_max=3):
    """
    æ¸…ç†æ—§æ¨¡å‹ï¼Œåªä¿ç•™æœ€æ–°çš„ keep_max ä¸ªç‰ˆæœ¬çš„æ¨¡å‹æ–‡ä»¶ (.pth å’Œ .pt)ã€‚
    """
    path = "."
    # åŒ¹é…æ¨¡å‹æ–‡ä»¶çš„æ­£åˆ™ï¼Œä¾‹å¦‚: model_10_5x128_20c.pth
    # è¿™é‡Œçš„æ­£åˆ™è¦è¶³å¤Ÿå®½å®¹ä»¥åŒ¹é… .pth å’Œ .pt
    pattern = re.compile(r"model_(\d+)_.*")

    # 1. æ‰«ææ‰€æœ‰æ¨¡å‹æ–‡ä»¶å¹¶æŒ‰ epoch åˆ†ç»„
    found_models = {}  # æ ¼å¼: {epoch: [file_path1, file_path2]}

    for f in os.listdir(path):
        # åªå¤„ç†æ–‡ä»¶
        if not os.path.isfile(os.path.join(path, f)):
            continue

        match = pattern.match(f)
        if match:
            # æ’é™¤ candidate æ¨¡å‹ï¼Œåªå¤„ç†æ­£å¼çš„ model_
            if "candidate" in f:
                continue

            epoch = int(match.group(1))
            if epoch not in found_models:
                found_models[epoch] = []
            found_models[epoch].append(f)

    # 2. å¦‚æœæ€»ç‰ˆæœ¬æ•°å°‘äºä¿ç•™æ•°ï¼Œä¸éœ€è¦æ¸…ç†
    if len(found_models) <= keep_max:
        return

    # 3. æŒ‰ epoch ä»å°åˆ°å¤§æ’åº
    sorted_epochs = sorted(found_models.keys())

    # 4. æ‰¾å‡ºéœ€è¦åˆ é™¤çš„ epoch (å³é™¤äº†æœ€å keep_max ä¸ªä¹‹å¤–çš„æ‰€æœ‰ epoch)
    epochs_to_delete = sorted_epochs[:-keep_max]

    # 5. æ‰§è¡Œåˆ é™¤
    print(f"\n[æ¸…ç†] æ­£åœ¨ç§»é™¤æ—§æ¨¡å‹æƒé‡ (ä¿ç•™æœ€è¿‘ {keep_max} ä¸ªç‰ˆæœ¬)...")
    for epoch in epochs_to_delete:
        files = found_models[epoch]
        for file_name in files:
            try:
                os.remove(file_name)
                print(f"  - å·²åˆ é™¤: {file_name}")
            except Exception as e:
                print(f"  - åˆ é™¤å¤±è´¥ {file_name}: {e}")


def save_model(model, epoch, args):
    num_channels = args['num_channels']
    base_filename = f"model_{epoch}_{args['num_res_blocks']}x{args['num_hidden']}_{num_channels}c"
    model_path_pth = f"{base_filename}.pth"
    model_path_pt = f"{base_filename}.pt"
    torch.save(model.state_dict(), model_path_pth)
    print(f"æ¨¡å‹ {model_path_pth} å·²ä¿å­˜ã€‚")
    model.eval()
    try:
        traced_script_module = torch.jit.trace(model,
                                               torch.rand(1, num_channels, args['board_size'], args['board_size']).to(
                                                   device))
        traced_script_module.save(model_path_pt)
        print(f"TorchScriptæ¨¡å‹ {model_path_pt} å·²æˆåŠŸå¯¼å‡ºã€‚")
    except Exception as e:
        print(f"ã€é”™è¯¯ã€‘å¯¼å‡ºTorchScriptæ¨¡å‹å¤±è´¥: {e}")

    # ==================== æ–°å¢ä»£ç  ====================
    # 3. è°ƒç”¨æ¸…ç†å‡½æ•°ï¼Œç§»é™¤è¿‡æœŸçš„æ—§æƒé‡
    remove_old_models(args, keep_max=3)
    # ================================================


def find_latest_model_file():
    path, max_epoch, latest_file_info, latest_mtime = ".", -1, None, -1
    pattern = re.compile(r"model_(\d+)_(\d+)x(\d+)_(\d+)c\.pth")
    for f in os.listdir(path):
        match = pattern.match(f)
        if match:
            epoch, mtime = int(match.group(1)), os.path.getmtime(os.path.join(path, f))
            if epoch > max_epoch or (epoch == max_epoch and mtime > latest_mtime):
                max_epoch, latest_mtime = epoch, mtime
                latest_file_info = {'path': f, 'epoch': epoch, 'res_blocks': int(match.group(2)),
                                    'hidden_units': int(match.group(3)), 'channels': int(match.group(4))}
    return latest_file_info


class Coach:
    def __init__(self, model, args):
        self.model, self.args = model, args
        self.scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))
        self.self_play_data = deque(maxlen=self.args['data_max_size'])
        self.expert_data = deque(maxlen=self.args.get('expert_data_max_size', 50000))

    def train(self, model_to_train, optimizer, scheduler=None):
        model_to_train.train()
        policy_losses, value_losses = [], []
        for _ in tqdm.tqdm(range(self.args.get('training_steps_per_iteration', 500)), desc="è®­ç»ƒæ¨¡å‹ Steps"):
            expert_samples, self_play_samples = [], []
            total_batch_size, expert_ratio = self.args['batch_size'], self.args.get('expert_data_ratio', 0.25)
            expert_batch_size = int(total_batch_size * expert_ratio) if len(self.expert_data) > 0 else 0
            expert_batch_size = min(len(self.expert_data), expert_batch_size)
            if expert_batch_size > 0: expert_samples = random.sample(self.expert_data, expert_batch_size)
            self_play_batch_size = total_batch_size - expert_batch_size
            if len(self.self_play_data) < self_play_batch_size: continue
            self_play_samples = random.sample(self.self_play_data, self_play_batch_size)
            batch = expert_samples + self_play_samples
            if not batch: continue
            augmented_batch = []
            for item in batch:
                # æ ¹æ®itemé•¿åº¦åˆ¤æ–­æ˜¯ä¸“å®¶æ•°æ®è¿˜æ˜¯è‡ªå¯¹å¼ˆæ•°æ®
                if len(item) == 4:
                    state, policy, value, _ = item  # è§£åŒ…4ä¸ªå…ƒç´ ï¼Œå¿½ç•¥æœ€åä¸€ä¸ª
                else:
                    state, policy, value = item  # æ­£å¸¸è§£åŒ…3ä¸ªå…ƒç´ 

                # åç»­çš„æ•°æ®å¢å¼ºé€»è¾‘ä¿æŒä¸å˜
                if np.any(policy):
                    for aug_s, aug_p in get_augmented_data(state, policy, self.args['board_size'],
                                                           self.args['num_channels']):
                        augmented_batch.append((aug_s, aug_p, value))
                else:
                    augmented_batch.append((np.array(state).reshape(self.args['num_channels'], self.args['board_size'],
                                                                    self.args['board_size']), policy, value))
            states, target_policies, target_values = zip(*augmented_batch)
            states = torch.tensor(np.array(states), dtype=torch.float32).to(device)
            target_policies = torch.tensor(np.array(target_policies), dtype=torch.float32).to(device)
            target_values = torch.tensor(np.array(target_values), dtype=torch.float32).unsqueeze(1).to(device)
            optimizer.zero_grad(set_to_none=True)
            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
                pred_log_policies, pred_values = model_to_train(states)
                policy_mask = torch.any(target_policies, dim=1)
                policy_loss = -torch.sum(
                    target_policies[policy_mask] * pred_log_policies[policy_mask]) / policy_mask.sum() if torch.any(
                    policy_mask) else torch.tensor(0.0).to(device)
                value_loss = F.mse_loss(pred_values, target_values)
                total_loss = policy_loss + self.args['value_loss_weight'] * value_loss
            policy_losses.append(policy_loss.item());
            value_losses.append(value_loss.item())
            self.scaler.scale(total_loss).backward();
            self.scaler.step(optimizer);
            self.scaler.update()
            if scheduler: scheduler.step()
        return (np.mean(policy_losses) if policy_losses else 0), (np.mean(value_losses) if value_losses else 0)

    def learn(self):
        self_play_data_file = 'self_play_data.pkl'
        if os.path.exists(self_play_data_file):
            try:
                with open(self_play_data_file, 'rb') as f:
                    self.self_play_data.extend(pickle.load(f))
                print(f"[æ•°æ®åŠ è½½] æˆåŠŸä» '{self_play_data_file}' åŠ è½½äº† {len(self.self_play_data)} æ¡è‡ªå¯¹å¼ˆæ•°æ®ã€‚")
            except Exception as e:
                print(f"[è­¦å‘Š] åŠ è½½ '{self_play_data_file}' å¤±è´¥: {e}")
        human_data_file = 'human_games.pkl'
        if os.path.exists(human_data_file):
            try:
                with open(human_data_file, 'rb') as f:
                    expert_data_loaded = pickle.load(f)
                self.expert_data.clear();
                self.expert_data.extend(expert_data_loaded)
                print(f"[æ•°æ®åŠ è½½] æˆåŠŸä» '{human_data_file}' åŠ è½½äº† {len(self.expert_data)} æ¡ä¸“å®¶æ•°æ®ã€‚")
            except Exception as e:
                print(f"[è­¦å‘Š] åŠ è½½ '{human_data_file}' å¤±è´¥: {e}")
        print(f"\n--- å¯åŠ¨æ—¶æ€»æ•°æ®: {len(self.self_play_data)} (è‡ªå¯¹å¼ˆ) + {len(self.expert_data)} (ä¸“å®¶) ---\n")

        best_model_info = find_latest_model_file()
        current_model_epoch = best_model_info['epoch']

        # --- æ ¸å¿ƒä¿®æ­£ï¼šæ¢å¤æ­¤å˜é‡çš„å®šä¹‰ ---
        num_successful_promotions_to_achieve = self.args['num_iterations']
        target_epoch = current_model_epoch + num_successful_promotions_to_achieve

        print(
            f"è®­ç»ƒå¯åŠ¨ï¼šå½“å‰æ¨¡å‹è½®æ¬¡ {current_model_epoch}ï¼Œç›®æ ‡è½®æ¬¡ {target_epoch} (éœ€è¦ {num_successful_promotions_to_achieve} æ¬¡æˆåŠŸæ™‹å‡)")

        attempt_num, elo_best_model, model_was_promoted = 0, 1500, True
        while current_model_epoch < target_epoch:
            attempt_num += 1
            promotions_needed = target_epoch - current_model_epoch
            print(
                f"\n{'=' * 20} å°è¯•å‘¨æœŸ: {attempt_num} | ç›®æ ‡: model_{current_model_epoch + 1} (è¿˜éœ€ {promotions_needed} æ¬¡æ™‹å‡) {'=' * 20}")
            best_model_info = find_latest_model_file()
            best_model_path_pt = best_model_info['path'].replace('.pth', '.pt')
            cpp_args = self.args.copy()
            if model_was_promoted:
                print(f"æ­¥éª¤1: æ¨¡å‹åˆšæ™‹å‡æˆ–é¦–æ¬¡è¿è¡Œï¼Œæ‰§è¡Œä¸€è½®å®Œæ•´çš„è‡ªå¯¹å¼ˆ ({cpp_args['num_selfPlay_episodes']} å±€)...")
            else:
                small_episodes = max(1, int(
                    cpp_args['num_selfPlay_episodes'] * self.args.get('failed_selfplay_ratio', 0.1)))
                cpp_args['num_selfPlay_episodes'] = small_episodes
                print(f"æ­¥éª¤1: ä¸Šæ¬¡å°è¯•æœªæ™‹å‡ï¼Œæ‰§è¡Œä¸€è½®å°è§„æ¨¡å¢é‡è‡ªå¯¹å¼ˆ ({small_episodes} å±€)...")
            print(f"   ä½¿ç”¨æ¨¡å‹: '{best_model_path_pt}'")
            final_data_queue = queue.Queue()
            cpp_mcts_engine.run_parallel_self_play(best_model_path_pt, device.type == 'cuda', final_data_queue,
                                                   cpp_args)

            games_processed, good_steps_collected, bad_steps_discarded, policy_entropies = 0, 0, 0, []
            enable_filtering = self.args.get('filter_zero_policy_data', True)
            discarded_samples_for_debug = []
            while not final_data_queue.empty():
                try:
                    result = final_data_queue.get_nowait()
                    if result.get("type") == "data":
                        games_processed += 1
                        for state, policy, value in result.get("data", []):
                            if not enable_filtering or np.any(policy):
                                self.self_play_data.append((state, policy, value));
                                good_steps_collected += 1
                                p_vec = np.array(policy);
                                p_vec = p_vec[p_vec > 0]
                                if p_vec.size > 0: policy_entropies.append(-np.sum(p_vec * np.log2(p_vec)))
                            else:
                                bad_steps_discarded += 1
                                discarded_samples_for_debug.append(state)
                except queue.Empty:
                    break

            print(
                f"æ•°æ®å¤„ç†å®Œæˆï¼æœ¬è½®å…±å¤„ç† {games_processed} å±€, æ”¶é›†åˆ° {good_steps_collected} ä¸ªæœ‰æ•ˆæ­¥éª¤, ä¸¢å¼ƒ {bad_steps_discarded} ä¸ªã€‚")
            if bad_steps_discarded > 0 and discarded_samples_for_debug:
                # è§£ææ‰€æœ‰è¢«ä¸¢å¼ƒæ•°æ®çš„æ­¥æ•°
                all_move_numbers = [get_move_number_from_state(s, self.args) for s in discarded_samples_for_debug]
                # ä½¿ç”¨Counterç»Ÿè®¡æ¯ä¸ªæ­¥æ•°å‡ºç°çš„é¢‘ç‡
                move_counts = Counter(all_move_numbers)
                # æŒ‰æ­¥æ•°æ’åºåè¾“å‡º
                sorted_counts = sorted(move_counts.items())

                print(f"  - [è°ƒè¯•ä¿¡æ¯] è¢«ä¸¢å¼ƒæ•°æ®çš„æ­¥æ•°é¢‘ç‡ç»Ÿè®¡ (æ­¥æ•°: æ¬¡æ•°): {sorted_counts}")
            print(f"å½“å‰è‡ªå¯¹å¼ˆæ± å¤§å°: {len(self.self_play_data)}, ä¸“å®¶æ± å¤§å°: {len(self.expert_data)}")
            if len(self.self_play_data) + len(self.expert_data) < self.args['batch_size']:
                print("è­¦å‘Šï¼šæ€»æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æœ¬æ¬¡è®­ç»ƒã€‚");
                model_was_promoted = False;
                continue

            promotion_achieved = False
            for cand_idx in range(self.args.get('num_candidates_to_train', 1)):
                print(
                    f"\n{'--' * 15} æ­£åœ¨å°è¯•ç¬¬ {cand_idx + 1} / {self.args.get('num_candidates_to_train', 1)} ä¸ªå€™é€‰æ¨¡å‹ {'--' * 15}")
                print("\næ­¥éª¤3.1: è®­ç»ƒå€™é€‰æ¨¡å‹...")
                cand_model = ExtendedConnectNet(board_size=self.args['board_size'],
                                                num_res_blocks=self.args['num_res_blocks'],
                                                num_hidden=self.args['num_hidden'],
                                                num_channels=self.args['num_channels']).to(device)
                cand_model.load_state_dict(self.model.state_dict())
                optimizer = optim.Adam(cand_model.parameters(), lr=self.args['learning_rate'],
                                       weight_decay=self.args.get('weight_decay', 0.0001))
                avg_p_loss, avg_v_loss = self.train(cand_model, optimizer)
                print(f"  - è®­ç»ƒæŸå¤±: Policy Loss={avg_p_loss:.4f}, Value Loss={avg_v_loss:.4f}")
                print("\næ­¥éª¤3.2: è¯„ä¼°å€™é€‰æ¨¡å‹ vs. æœ€ä¼˜æ¨¡å‹...")
                candidate_model_path_pt = f"candidate_{cand_idx}.pt"
                cand_model.eval();
                traced_script_module = torch.jit.trace(cand_model,
                                                       torch.rand(1, self.args['num_channels'], self.args['board_size'],
                                                                  self.args['board_size']).to(device));
                traced_script_module.save(candidate_model_path_pt)
                games_per_side = self.args.get('num_eval_games', 20) // 2
                eval_args = self.args.copy();
                eval_args['num_eval_games'] = games_per_side;
                eval_args['num_eval_simulations'] = self.args['num_searches']
                res1 = cpp_mcts_engine.run_parallel_evaluation(best_model_path_pt, candidate_model_path_pt,
                                                               device.type == 'cuda', eval_args, mode=2)
                res2 = cpp_mcts_engine.run_parallel_evaluation(best_model_path_pt, candidate_model_path_pt,
                                                               device.type == 'cuda', eval_args, mode=1)
                total_new_wins = res1.get("model2_wins", 0) + res2.get("model2_wins", 0);
                total_old_wins = res1.get("model1_wins", 0) + res2.get("model1_wins", 0);
                total_draws = res1.get("draws", 0) + res2.get("draws", 0)
                total_games = total_new_wins + total_old_wins + total_draws
                win_rate = total_new_wins / total_games if total_games > 0 else 0
                print("\nè¯„ä¼°æ€»ç»“:")
                print(
                    f"  - æ–°æ¨¡å‹æ‰§é»‘æ—¶ (æ–° vs æ—§): {res1.get('model2_wins', 0)} èƒœ / {res1.get('model1_wins', 0)} è´Ÿ / {res1.get('draws', 0)} å¹³")
                print(
                    f"  - æ–°æ¨¡å‹æ‰§ç™½æ—¶ (æ—§ vs æ–°): {res2.get('model2_wins', 0)} èƒœ / {res2.get('model1_wins', 0)} è´Ÿ / {res2.get('draws', 0)} å¹³")
                print(f"  - ç»¼åˆæˆ˜ç»© (æ–° vs æ—§): {total_new_wins} èƒœ / {total_old_wins} è´Ÿ / {total_draws} å¹³")
                print(f"  - æ–°æ¨¡å‹ç»¼åˆèƒœç‡: {win_rate:.2%}")
                expected_win_rate_candidate = 1 / (1 + 10 ** ((elo_best_model - elo_best_model) / 400));
                actual_score_candidate = total_new_wins + 0.5 * total_draws;
                expected_score_candidate = expected_win_rate_candidate * total_games
                new_elo_candidate = elo_best_model + self.args.get('elo_k_factor', 32) * (
                            actual_score_candidate - expected_score_candidate)
                print(f"  - Elo è¯„çº§: BestNet ({elo_best_model:.0f}) vs Candidate ({elo_best_model:.0f})");
                print(
                    f"  - Elo å˜åŒ–: Candidate Elo -> {new_elo_candidate:.0f} ({new_elo_candidate - elo_best_model:+.0f})")
                if os.path.exists(candidate_model_path_pt): os.remove(candidate_model_path_pt)
                if win_rate >= self.args.get('promotion_win_rate', 0.55):
                    next_model_epoch = current_model_epoch + 1
                    print(
                        f"ã€æ¨¡å‹æ™‹å‡ã€‘å€™é€‰ {cand_idx + 1} èƒœç‡è¾¾æ ‡ï¼Œå°†å…¶ä¿å­˜ä¸º model_{next_model_epoch} å¹¶è®¾ä¸ºæ–°çš„æœ€ä¼˜æ¨¡å‹ã€‚")
                    elo_best_model = new_elo_candidate;
                    self.model.load_state_dict(cand_model.state_dict());
                    save_model(self.model, next_model_epoch, self.args)
                    current_model_epoch = next_model_epoch;
                    promotion_achieved = True;
                    break
                else:
                    print(f"ã€æ¨¡å‹ä¸¢å¼ƒã€‘å€™é€‰ {cand_idx + 1} èƒœç‡æœªè¾¾æ ‡ã€‚")
            model_was_promoted = promotion_achieved
            if not model_was_promoted: print(f"\n--- æœ¬æ¬¡å°è¯•æœªèƒ½æ™‹å‡ï¼Œå°†ç»§ç»­å°è¯•å‡»è´¥ model_{current_model_epoch} ---")
            print(f"\n{'=' * 20} å°è¯•å‘¨æœŸ {attempt_num} æ€»ç»“ {'=' * 20}")
            print("æ€§èƒ½æŒ‡æ ‡:");
            print(f"  - å½“å‰æœ€ä¼˜æ¨¡å‹ Elo: {elo_best_model:.0f} (model_{current_model_epoch})")
            print("è¡Œä¸ºç»Ÿè®¡ (æ¥è‡ªæœ¬è½®è‡ªå¯¹å¼ˆ):");
            avg_entropy = np.mean(policy_entropies) if policy_entropies else 0
            print(f"  - å¹³å‡MCTSç­–ç•¥ç†µ: {avg_entropy:.3f} bits");
            print(f"{'=' * 56}")
            clear_windows_memory()

        print(
            f"\nè®­ç»ƒç›®æ ‡è¾¾æˆï¼å·²æˆåŠŸæ™‹å‡ {num_successful_promotions_to_achieve} ä»£æ–°æ¨¡å‹ï¼Œæœ€ç»ˆæ¨¡å‹ä¸º model_{current_model_epoch}ã€‚")

    def evaluate_models(self, model1_info, model2_info):
        print(f"\n------ å¼€å§‹åˆ†ç»„è¯Šæ–­å¼è¯„ä¼° (C++ å¼•æ“é©±åŠ¨) ------")
        if not model1_info or not model2_info: print("è¯„ä¼°ç¼ºå°‘å¿…è¦çš„æ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡è¯„ä¼°ã€‚"); return
        model1_pt_path, model2_pt_path = model1_info['path'].replace('.pth', '.pt'), model2_info['path'].replace('.pth',
                                                                                                                 '.pt')
        if not os.path.exists(model1_pt_path) or not os.path.exists(model2_pt_path): print(
            "è¯„ä¼°ç¼ºå°‘å¿…è¦çš„.ptæ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡è¯„ä¼°ã€‚"); return
        print(f"è¯„ä¼°æ¨¡å‹ (æ—§): {model1_pt_path}");
        print(f"è¯„ä¼°æ¨¡å‹ (æ–°): {model2_pt_path}")
        use_gpu, total_games, games_per_side = (device.type == 'cuda'), self.args.get('num_eval_games',
                                                                                      100), self.args.get(
            'num_eval_games', 100) // 2
        if games_per_side == 0: print("è¯„ä¼°å±€æ•°è¿‡å°‘ï¼Œæ— æ³•è¿›è¡Œåˆ†ç»„è¯„ä¼°ã€‚"); return
        eval_args = {k: self.args.get(k) for k in
                     ['num_searches', 'num_cpu_threads', 'C', 'mcts_batch_size', 'board_size', 'num_rounds',
                      'history_steps', 'num_channels', 'enable_territory_heuristic', 'territory_heuristic_weight',
                      'enable_territory_penalty', 'territory_penalty_strength', 'enable_ineffective_connection_penalty',
                      'ineffective_connection_penalty_factor']}
        eval_args['num_eval_games'] = games_per_side;
        eval_args['num_eval_simulations'] = eval_args['num_searches']
        print(f"\n[å®éªŒä¸€] æ–°æ¨¡å‹æ‰§é»‘ï¼Œè¿›è¡Œ {games_per_side} å±€...")
        results1 = cpp_mcts_engine.run_parallel_evaluation(model1_pt_path, model2_pt_path, use_gpu, eval_args, mode=2)
        new_as_p1_wins, old_as_p2_wins, draws1 = results1.get("model2_wins", 0), results1.get("model1_wins",
                                                                                              0), results1.get("draws",
                                                                                                               0)
        print(f"\n[å®éªŒäºŒ] æ—§æ¨¡å‹æ‰§é»‘ï¼Œè¿›è¡Œ {games_per_side} å±€...")
        results2 = cpp_mcts_engine.run_parallel_evaluation(model1_pt_path, model2_pt_path, use_gpu, eval_args, mode=1)
        old_as_p1_wins, new_as_p2_wins, draws2 = results2.get("model1_wins", 0), results2.get("model2_wins",
                                                                                              0), results2.get("draws",
                                                                                                               0)
        total_new_wins, total_old_wins, total_draws = new_as_p1_wins + new_as_p2_wins, old_as_p1_wins + old_as_p2_wins, draws1 + draws2
        overall_win_rate = total_new_wins / total_games if total_games > 0 else 0
        print("\n------ è¯Šæ–­è¯„ä¼°ç»“æœ ------")
        print(f"æ–°æ¨¡å‹æ‰§å…ˆæ‰‹æ—¶ï¼Œæˆ˜ç»© (æ–° vs æ—§ | èƒœ/è´Ÿ/å¹³): {new_as_p1_wins} / {old_as_p2_wins} / {draws1}")
        print(f"æ—§æ¨¡å‹æ‰§å…ˆæ‰‹æ—¶ï¼Œæˆ˜ç»© (æ—§ vs æ–° | èƒœ/è´Ÿ/å¹³): {old_as_p1_wins} / {new_as_p2_wins} / {draws2}")
        print("---------------------------------");
        print(f"ç»¼åˆæˆ˜ç»© - æ–° vs æ—§ (èƒœ/è´Ÿ/å¹³): {total_new_wins} / {total_old_wins} / {total_draws}");
        print(f"æ–°æ¨¡å‹ç»¼åˆèƒœç‡: {overall_win_rate:.2%}")
        if games_per_side > 0 and (new_as_p1_wins / games_per_side) > 0.9 and (old_as_p1_wins / games_per_side) > 0.9:
            print("\nã€è¯Šæ–­ç»“è®ºã€‘: AIå·²å‘ç°å¹¶æŒæ¡äº† 'å…ˆæ‰‹å¿…èƒœ' ç­–ç•¥ã€‚")
        elif overall_win_rate > self.args.get('eval_win_rate', 0.52):
            print("\nã€è¯Šæ–­ç»“è®ºã€‘: æ–°æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼ğŸ‘")
        else:
            print("\nã€è¯Šæ–­ç»“è®ºã€‘: æ–°æ¨¡å‹æå‡ä¸æ˜æ˜¾æˆ–æ²¡æœ‰æå‡ã€‚")


if __name__ == '__main__':
    history_channels = (args.get('history_steps', 0) + 1) * 4
    meta_channels = 4
    total_channels = history_channels + meta_channels
    args['num_channels'] = total_channels
    print("=" * 50);
    print("MyAIChess é…ç½®åŠ è½½å®Œæˆ");
    print(f"å†å²æ­¥æ•°: {args.get('history_steps', 0)}");
    print(f"è®¡ç®—å‡ºçš„æ€»è¾“å…¥é€šé“æ•°: {args['num_channels']}")
    print("=" * 50);
    print(f"å°†è¦ä½¿ç”¨çš„è®¾å¤‡ (ä¸»è¿›ç¨‹/è®­ç»ƒ): {device}")
    latest_model_info = find_latest_model_file()
    current_model = ExtendedConnectNet(board_size=args['board_size'], num_res_blocks=args['num_res_blocks'],
                                       num_hidden=args['num_hidden'], num_channels=args['num_channels']).to(device)
    if latest_model_info is None:
        print("æœªæ‰¾åˆ°ä»»ä½•å·²æœ‰æ¨¡å‹ï¼Œå°†ä»ç¬¬ 1 è½®å¼€å§‹å…¨æ–°è®­ç»ƒã€‚");
        print("æ­£åœ¨åˆ›å»ºå¹¶ä¿å­˜åˆå§‹éšæœºæ¨¡å‹ (model_0)...")
        save_model(current_model, 0, args)
    else:
        print(f"æ‰¾åˆ°æœ€æ–°æ¨¡å‹: {latest_model_info['path']} (ç¬¬ {latest_model_info['epoch']} è½®)")
        config_blocks, config_hidden, config_channels = args['num_res_blocks'], args['num_hidden'], args['num_channels']
        is_same_architecture = (latest_model_info['res_blocks'] == config_blocks and latest_model_info[
            'hidden_units'] == config_hidden and latest_model_info['channels'] == config_channels)
        if is_same_architecture:
            print("æ¨¡å‹ç»“æ„ä¸å½“å‰é…ç½®ä¸€è‡´ï¼Œç›´æ¥åŠ è½½æƒé‡ç»§ç»­è®­ç»ƒã€‚")
            try:
                current_model.load_state_dict(torch.load(latest_model_info['path'], map_location=device));
                print("æƒé‡åŠ è½½æˆåŠŸï¼")
            except Exception as e:
                print(f"åŠ è½½æƒé‡å¤±è´¥: {e}ï¼Œå°†ä»éšæœºæƒé‡å¼€å§‹ã€‚")
        else:
            print("æ¨¡å‹ç»“æ„ä¸å½“å‰é…ç½®ä¸ä¸€è‡´ï¼Œå°†æ‰§è¡Œè‡ªåŠ¨è¿ç§»å­¦ä¹ ã€‚")
            try:
                current_model = transfer_weights(current_model, latest_model_info['path'])
                print("ä¸ºè¿ç§»å­¦ä¹ åçš„æ–°æ¨¡å‹åˆ›å»ºåŒ¹é…çš„ .pt æ–‡ä»¶...");
                save_model(current_model, latest_model_info['epoch'], args)
            except Exception as e:
                print(f"è¿ç§»å­¦ä¹ å¤±è´¥: {e}ï¼Œå°†ä»éšæœºæƒé‡å¼€å§‹è®­ç»ƒæ–°ç»“æ„æ¨¡å‹ã€‚")

    coach = Coach(current_model, args)
    coach.learn()

    print("\nè®­ç»ƒå…¨éƒ¨å®Œæˆï¼Œæ­£åœ¨ä¿å­˜è‡ªå¯¹å¼ˆç»éªŒæ± ...")
    try:
        with open('self_play_data.pkl', 'wb') as f:
            pickle.dump(coach.self_play_data, f)
        print(f"æˆåŠŸå°† {len(coach.self_play_data)} æ¡è‡ªå¯¹å¼ˆæ•°æ®ä¿å­˜åˆ° 'self_play_data.pkl'ã€‚")
    except Exception as e:
        print(f"[é”™è¯¯] ä¿å­˜è‡ªå¯¹å¼ˆç»éªŒæ± å¤±è´¥: {e}")
    print("\næ­£åœ¨æ‰‹åŠ¨æ¸…ç†å†…å­˜...")
    if 'coach' in locals():
        if hasattr(coach, 'self_play_data'): coach.self_play_data.clear()
        if hasattr(coach, 'expert_data'): coach.expert_data.clear()
    print("å†…å­˜ä¸­çš„ç»éªŒæ± å·²æ¸…ç©ºã€‚")
    import gc

    gc.collect()
    clear_windows_memory()
    print("å†…å­˜æ¸…ç†å®Œæˆã€‚ç¨‹åºå³å°†é€€å‡ºã€‚")