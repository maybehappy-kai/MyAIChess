# file: coach.py (FINAL, BATCHED MCTS VERSION)
import torch
import torch.nn.functional as F
import torch.optim as optim
import queue
import threading
import numpy as np
import tqdm
import random
import os
import re
from torch.optim.lr_scheduler import CosineAnnealingLR

from neural_net import ExtendedConnectNet
from config import args
import cpp_mcts_engine
from collections import deque  # <-- æ–°å¢è¿™ä¸€è¡Œ
import platform
import ctypes


# å®šä¹‰ä¸€ä¸ªä»…åœ¨Windowsä¸‹ç”Ÿæ•ˆçš„å†…å­˜æ¸…ç†å‡½æ•°
def clear_windows_memory():
    if platform.system() == "Windows":
        try:
            # è°ƒç”¨Windows APIæ¥å¼ºåˆ¶æ¸…ç†å½“å‰è¿›ç¨‹çš„å†…å­˜å·¥ä½œé›†
            ctypes.windll.psapi.EmptyWorkingSet(ctypes.windll.kernel32.GetCurrentProcess())
            print("[System] Windows memory working set has been cleared.")
        except Exception as e:
            print(f"[System] Failed to clear Windows memory working set: {e}")


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def find_latest_model_file():
    path = "."
    max_epoch = 0
    latest_file = None
    pattern = re.compile(r"model_(\d+)\.pth")
    for f in os.listdir(path):
        match = pattern.match(f)
        if match:
            epoch = int(match.group(1))
            if epoch > max_epoch:
                max_epoch = epoch
                latest_file = f
    start_epoch = max_epoch + 1 if latest_file else 1
    return latest_file, start_epoch


# ==================== è¿™æ˜¯å”¯ä¸€éœ€è¦ä¿®æ”¹çš„åœ°æ–¹ ====================
# æ–°çš„ã€ä¸ºæ‰¹å¤„ç†MCTSä¼˜åŒ–çš„æ¨ç†æœåŠ¡å™¨
def inference_server_func(model, device, job_q, result_q, stop_event, board_size):
    model.eval()
    with torch.no_grad():
        while not stop_event.is_set():
            try:
                # 1. ä¸€æ¬¡åªè·å–ä¸€ä¸ªâ€œå·¥ä½œåŒ…â€ï¼Œè¿™ä¸ªåŒ…é‡ŒåŒ…å«äº†æ•´ä¸ªæ‰¹æ¬¡
                #    è®¾ç½®ä¸€ä¸ªè¶…æ—¶ï¼Œä»¥ä¾¿èƒ½å®šæœŸæ£€æŸ¥ stop_event
                request_id, state_batch = job_q.get(timeout=1.0)

                # 2. state_batch ç°åœ¨æ˜¯ä¸€ä¸ªçŠ¶æ€åˆ—è¡¨ï¼Œç›´æ¥è½¬æ¢æˆnumpyæ•°ç»„
                state_tensor = torch.tensor(np.array(state_batch), device=device, dtype=torch.float32)

                # ç¡®ä¿å¼ é‡å½¢çŠ¶æ­£ç¡® (B, C, H, W)
                state_tensor = state_tensor.view(-1, 6, board_size, board_size)

                # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è¿›è¡Œæ¨ç†
                with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
                    log_policies, values = model(state_tensor)

                # å°†torchå¼ é‡è½¬ä¸ºPythonåˆ—è¡¨ï¼Œæ–¹ä¾¿åç»­å¤„ç†
                # C++ç«¯å°†æ¥æ”¶åˆ° [[p1,p2...], [p1,p2...]] å’Œ [v1, v2...]
                policies = torch.exp(log_policies).cpu().numpy().tolist()
                values = values.squeeze(-1).cpu().numpy().tolist()

                # 3. å°†æ•´ä¸ªæ‰¹æ¬¡çš„ç»“æœæ‰“åŒ…åï¼Œä¸€æ¬¡æ€§æ”¾å›ç»“æœé˜Ÿåˆ—
                result_q.put((request_id, policies, values))

            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºæ˜¯æ­£å¸¸ç°è±¡ï¼Œç»§ç»­å¾ªç¯ï¼Œæ£€æŸ¥stop_event
                continue


# =============================================================


class Coach:
    def __init__(self, model, args):
        self.model = model
        self.args = args
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.args['learning_rate'], weight_decay=0.0001)
        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=self.args['num_epochs'])
        self.scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))
        self.training_data = deque(maxlen=self.args['data_max_size'])  # <-- ä¿®æ”¹è¿™ä¸€è¡Œ

    def train(self):
        self.model.train()
        if len(self.training_data) < self.args['batch_size']: return
        batch = random.sample(self.training_data, self.args['batch_size'])
        states, target_policies, target_values = zip(*batch)
        states = torch.tensor(np.array(states), dtype=torch.float32).to(device)
        target_policies = torch.tensor(np.array(target_policies), dtype=torch.float32).to(device)
        target_values = torch.tensor(np.array(target_values), dtype=torch.float32).unsqueeze(1).to(device)
        states = states.view(-1, 6, self.args['board_size'], self.args['board_size'])
        self.optimizer.zero_grad(set_to_none=True)
        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
            pred_log_policies, pred_values = self.model(states)
            policy_loss = -torch.sum(target_policies * pred_log_policies) / len(target_policies)
            value_loss = F.mse_loss(pred_values, target_values)
            total_loss = policy_loss + self.args['value_loss_weight'] * value_loss
        self.scaler.scale(total_loss).backward()
        self.scaler.step(self.optimizer)
        self.scaler.update()

    def learn(self, start_epoch=1):
        for i in range(start_epoch, start_epoch + self.args['num_iterations']):
            print(f"------ è¿­ä»£è½®æ¬¡: {i} ------")
            # --- vvv è¿™é‡Œæ˜¯éœ€è¦ä¿®æ”¹çš„C++å¼•æ“è°ƒç”¨éƒ¨åˆ† vvv ---

            # --- vvv è¿™é‡Œæ˜¯éœ€è¦ä¿®æ”¹çš„æ ¸å¿ƒåŒºåŸŸ vvv ---
            print("æ­¥éª¤1ï¼šå¯åŠ¨çº¯C++å¼•æ“è¿›è¡Œè‡ªæˆ‘å¯¹å¼ˆ (æ­¤è¿‡ç¨‹å°†é˜»å¡)...")
            final_data_queue = queue.Queue()

            cpp_args = {
                'num_selfPlay_episodes': self.args['num_selfPlay_episodes'],
                'num_cpu_threads': self.args['num_cpu_threads'],
                'num_searches': self.args['num_searches']
            }

            # ã€ä¿®æ”¹ã€‘è·å–å‰ä¸€è½®çš„æ¨¡å‹è·¯å¾„ï¼Œè€Œä¸æ˜¯å½“å‰è½®æ¬¡çš„
            model_path_pt = f"model_{i - 1}.pt"
            use_gpu = (device.type == 'cuda')

            print(f"[Python Coach] æŒ‡ç¤ºC++å¼•æ“ä½¿ç”¨æ¨¡å‹: {model_path_pt}")
            cpp_mcts_engine.run_parallel_self_play(
                model_path_pt,
                use_gpu,
                final_data_queue,
                cpp_args
            )
            # --- ^^^ ä¿®æ”¹ç»“æŸ ^^^ ---

            # 5. ä¸å†éœ€è¦åœæ­¢å’ŒåŠ å…¥æ¨ç†çº¿ç¨‹
            # stop_event.set()
            # server_thread.join()

            # --- ^^^ ä¿®æ”¹ç»“æŸ ^^^ ---

            print("\nè‡ªæˆ‘å¯¹å¼ˆå®Œæˆï¼æ­£åœ¨æ”¶é›†æ•°æ®...")
            # è¿™ä¸ªæ•°æ®æ”¶é›†é€»è¾‘æ˜¯æ­£ç¡®çš„
            with tqdm.tqdm(total=self.args['num_selfPlay_episodes'], desc="æ”¶é›†æ•°æ®") as pbar:
                games_processed = 0
                while games_processed < self.args['num_selfPlay_episodes']:
                    try:
                        result = final_data_queue.get(timeout=1.0)  # åŠ ä¸€ä¸ªè¶…æ—¶ä»¥é˜²ä¸‡ä¸€
                        if result.get("type") == "data":
                            self.training_data.extend(result.get("data", []))
                            games_processed += 1
                            pbar.update(1)
                    except queue.Empty:
                        print("\nè­¦å‘Šï¼šæ•°æ®é˜Ÿåˆ—ä¸ºç©ºï¼Œä½†è‡ªæˆ‘å¯¹å¼ˆå·²ç»“æŸã€‚å¯èƒ½æŸäº›å¯¹å±€æœªèƒ½ç”Ÿæˆæ•°æ®ã€‚")
                        break  # å¦‚æœé˜Ÿåˆ—ç©ºäº†ï¼Œå°±è·³å‡ºå¾ªç¯

            print(f"\nç»éªŒåº“å¤§å°: {len(self.training_data)}")
            # if len(self.training_data) > self.args['data_max_size']:
            # self.training_data = self.training_data[-self.args['data_max_size']:]

            print("\næ­¥éª¤2ï¼šè®­ç»ƒç¥ç»ç½‘ç»œ (ä½¿ç”¨GPU)...")
            if not self.training_data:
                print("ç»éªŒåº“ä¸ºç©ºï¼Œè·³è¿‡æœ¬è½®è®­ç»ƒã€‚")
            else:
                self.model.train()
                for _ in tqdm.tqdm(range(self.args['num_epochs']), desc="è®­ç»ƒæ¨¡å‹"):
                    self.train()

            self.scheduler.step()
            # --- vvv è¿™é‡Œæ˜¯æ‚¨éœ€è¦ä¿®æ”¹çš„æ ¸å¿ƒåŒºåŸŸ vvv ---

            # 1. ä¿å­˜æ ‡å‡†çš„ .pth æƒé‡æ–‡ä»¶ (è¿™è¡Œä»£ç ä¿æŒä¸å˜)
            model_path_pth = f"model_{i}.pth"
            torch.save(self.model.state_dict(), model_path_pth)
            print(f"æ¨¡å‹ {model_path_pth} å·²ä¿å­˜ã€‚")

            # 2. ã€æ–°å¢ä»£ç ã€‘å¯¼å‡ºå¯ä¾›C++ä½¿ç”¨çš„ TorchScript æ¨¡å‹
            model_path_pt = f"model_{i}.pt"
            self.model.eval()  # å¯¼å‡ºå‰ï¼Œå¿…é¡»å°†æ¨¡å‹åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼

            # åˆ›å»ºä¸€ä¸ªç¬¦åˆæ¨¡å‹è¾“å…¥çš„ç¤ºä¾‹å¼ é‡ã€‚
            # å½¢çŠ¶ä¸º (batch_size, channels, height, width)
            example_input = torch.rand(
                1,
                6,  # æ ¹æ®æ‚¨çš„Gomoku.cppï¼ŒçŠ¶æ€æœ‰6ä¸ªé€šé“
                self.args['board_size'],
                self.args['board_size']
            ).to(device)

            try:
                # ä½¿ç”¨ torch.jit.trace åŠŸèƒ½è¿½è¸ªæ¨¡å‹çš„è®¡ç®—å›¾
                traced_script_module = torch.jit.trace(self.model, example_input)
                # å°†è¿½è¸ªåˆ°çš„è®¡ç®—å›¾ä¿å­˜ä¸º .pt æ–‡ä»¶
                traced_script_module.save(model_path_pt)
                print(f"TorchScriptæ¨¡å‹ {model_path_pt} å·²æˆåŠŸå¯¼å‡ºï¼Œå¯ä¾›C++ä½¿ç”¨ã€‚")
            except Exception as e:
                print(f"ã€é”™è¯¯ã€‘å¯¼å‡ºTorchScriptæ¨¡å‹å¤±è´¥: {e}")

            # --- ^^^ ä¿®æ”¹ç»“æŸ ^^^ ---
            clear_windows_memory()
        print(f"\nè®­ç»ƒå®Œæˆï¼")

    # evaluate_models å‡½æ•°æ— éœ€æ”¹åŠ¨
    def evaluate_models(self, model1_path, model2_path):
        print(f"\n------ å¼€å§‹åˆ†ç»„è¯Šæ–­å¼è¯„ä¼° (C++ å¼•æ“é©±åŠ¨) ------")
        if not model1_path or not os.path.exists(model1_path) or \
                not model2_path or not os.path.exists(model2_path):
            print("è¯„ä¼°ç¼ºå°‘å¿…è¦çš„æ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡è¯„ä¼°ã€‚")
            return

        model1_pt_path = model1_path.replace('.pth', '.pt')  # æ—§æ¨¡å‹
        model2_pt_path = model2_path.replace('.pth', '.pt')  # æ–°æ¨¡å‹
        use_gpu = (device.type == 'cuda')

        total_games = self.args.get('num_eval_games', 50)
        games_per_side = total_games // 2
        if games_per_side == 0:
            print("è¯„ä¼°å±€æ•°è¿‡å°‘ï¼Œæ— æ³•è¿›è¡Œåˆ†ç»„è¯„ä¼°ã€‚")
            return

        print(f"è¯„ä¼°æ¨¡å‹ (æ—§): {model1_pt_path}")
        print(f"è¯„ä¼°æ¨¡å‹ (æ–°): {model2_pt_path}")

        base_eval_args = {
            'num_eval_games': games_per_side,
            'num_eval_simulations': self.args.get('num_eval_simulations', 20),
            'num_cpu_threads': self.args.get('num_cpu_threads', 12)
        }

        # --- å®éªŒä¸€ï¼šæ–°æ¨¡å‹æ‰§å…ˆæ‰‹ (Model 2) ---
        print(f"\n[å®éªŒä¸€] æ–°æ¨¡å‹æ‰§é»‘ï¼Œè¿›è¡Œ {games_per_side} å±€...")
        results1 = cpp_mcts_engine.run_parallel_evaluation(
            model1_pt_path, model2_pt_path, use_gpu, base_eval_args, mode=2
        )
        new_as_p1_wins = results1.get("model2_wins", 0)
        old_as_p2_wins = results1.get("model1_wins", 0)
        draws1 = results1.get("draws", 0)

        # --- å®éªŒäºŒï¼šæ—§æ¨¡å‹æ‰§å…ˆæ‰‹ (Model 1) ---
        print(f"\n[å®éªŒäºŒ] æ—§æ¨¡å‹æ‰§é»‘ï¼Œè¿›è¡Œ {games_per_side} å±€...")
        results2 = cpp_mcts_engine.run_parallel_evaluation(
            model1_pt_path, model2_pt_path, use_gpu, base_eval_args, mode=1
        )
        old_as_p1_wins = results2.get("model1_wins", 0)
        new_as_p2_wins = results2.get("model2_wins", 0)
        draws2 = results2.get("draws", 0)

        # --- æ±‡æ€»å’Œåˆ†æç»“æœ ---
        total_new_wins = new_as_p1_wins + new_as_p2_wins
        total_old_wins = old_as_p1_wins + old_as_p2_wins
        total_draws = draws1 + draws2

        print("\n------ è¯Šæ–­è¯„ä¼°ç»“æœ ------")
        print(f"æ–°æ¨¡å‹æ‰§å…ˆæ‰‹æ—¶ï¼Œæˆ˜ç»© (æ–° vs æ—§ | èƒœ/è´Ÿ/å¹³): {new_as_p1_wins} / {old_as_p2_wins} / {draws1}")
        print(f"æ—§æ¨¡å‹æ‰§å…ˆæ‰‹æ—¶ï¼Œæˆ˜ç»© (æ—§ vs æ–° | èƒœ/è´Ÿ/å¹³): {old_as_p1_wins} / {new_as_p2_wins} / {draws2}")
        print("---------------------------------")

        overall_win_rate = total_new_wins / (total_games) if total_games > 0 else 0
        print(f"ç»¼åˆæˆ˜ç»© - æ–° vs æ—§ (èƒœ/è´Ÿ/å¹³): {total_new_wins} / {total_old_wins} / {total_draws}")
        print(f"æ–°æ¨¡å‹ç»¼åˆèƒœç‡: {overall_win_rate:.2%}")

        if games_per_side > 0 and (new_as_p1_wins / games_per_side) > 0.9 and (old_as_p1_wins / games_per_side) > 0.9:
            print("\nã€è¯Šæ–­ç»“è®ºã€‘: AIå·²å‘ç°å¹¶æŒæ¡äº† 'å…ˆæ‰‹å¿…èƒœ' ç­–ç•¥ã€‚")
            print("ä¸‹ä¸€æ­¥å»ºè®®ï¼šåœ¨è‡ªå¯¹å¼ˆä¸­å¼•å…¥'ç‹„åˆ©å…‹é›·å™ªå£°'å’Œ'æ¸©åº¦é‡‡æ ·'æ¥æ‰“ç ´åƒµå±€ï¼Œæ¢ç´¢åæ‰‹è·èƒœçš„å¯èƒ½æ€§ã€‚")
        elif overall_win_rate > self.args.get('eval_win_rate', 0.52):
            print("\nã€è¯Šæ–­ç»“è®ºã€‘: æ–°æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼ğŸ‘")
        else:
            print("\nã€è¯Šæ–­ç»“è®ºã€‘: æ–°æ¨¡å‹æå‡ä¸æ˜æ˜¾æˆ–æ²¡æœ‰æå‡ï¼Œå¯èƒ½é™·å…¥äº†å±€éƒ¨æœ€ä¼˜ã€‚")


if __name__ == '__main__':
    print(f"å°†è¦ä½¿ç”¨çš„è®¾å¤‡ (ä¸»è¿›ç¨‹/è®­ç»ƒ): {device}")
    model_before_training, start_epoch = find_latest_model_file()
    model = ExtendedConnectNet(
        board_size=args['board_size'],
        num_res_blocks=args['num_res_blocks'],
        num_hidden=args['num_hidden']
    ).to(device)

    if model_before_training:
        try:
            print(f"æ‰¾åˆ°æœ€æ–°æ¨¡å‹ {model_before_training}ï¼Œå°†ä»ç¬¬ {start_epoch} è½®å¼€å§‹ç»§ç»­è®­ç»ƒ...")
            model.load_state_dict(torch.load(model_before_training, map_location=device))
            print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä»é›¶å¼€å§‹è®­ç»ƒã€‚")
            start_epoch = 1
            model_before_training = None
    else:
        # --- vvv è¿™é‡Œæ˜¯éœ€è¦ä¿®æ”¹çš„æ ¸å¿ƒåŒºåŸŸ vvv ---
        print("æœªæ‰¾åˆ°ä»»ä½•å·²æœ‰æ¨¡å‹ï¼Œå°†ä»ç¬¬ 1 è½®å¼€å§‹å…¨æ–°è®­ç»ƒã€‚")
        print("æ­£åœ¨åˆ›å»ºå¹¶ä¿å­˜åˆå§‹éšæœºæ¨¡å‹ (model_0)...")

        # ã€æ–°å¢ä»£ç ã€‘ä¸ºç¬¬ä¸€è½®è‡ªæˆ‘å¯¹å¼ˆå‡†å¤‡ä¸€ä¸ªâ€œç¬¬0ä»£â€æ¨¡å‹
        model.eval()
        example_input = torch.rand(1, 6, args['board_size'], args['board_size']).to(device)
        try:
            traced_script_module = torch.jit.trace(model, example_input)
            traced_script_module.save("model_0.pt")
            torch.save(model.state_dict(), "model_0.pth")
            print("åˆå§‹æ¨¡å‹ model_0.pt å’Œ model_0.pth å·²ä¿å­˜ã€‚")
        except Exception as e:
            print(f"ã€é”™è¯¯ã€‘åˆ›å»ºåˆå§‹æ¨¡å‹å¤±è´¥: {e}, ç¨‹åºæ— æ³•ç»§ç»­ã€‚")
            exit()  # å¦‚æœåˆå§‹æ¨¡å‹éƒ½åˆ›å»ºå¤±è´¥ï¼Œç›´æ¥é€€å‡º

        start_epoch = 1  # ç¡®è®¤ä»ç¬¬1è½®å¼€å§‹
        # --- ^^^ ä¿®æ”¹ç»“æŸ ^^^ ---

    coach = Coach(model, args)
    coach.learn(start_epoch=start_epoch)
    # ... (åç»­è¯„ä¼°ä»£ç ä¸å˜) ...
    model_after_training, _ = find_latest_model_file()
    if model_before_training and model_after_training != model_before_training:
        coach.evaluate_models(model_before_training, model_after_training)
    else:
        print("\næœªè¿›è¡Œæœ‰æ•ˆçš„æ–°ä¸€è½®è®­ç»ƒæˆ–æœªæ‰¾åˆ°æ—§æ¨¡å‹ï¼Œè·³è¿‡è¯„ä¼°ã€‚")

        # ====================== æ–°å¢çš„æ ¸å¿ƒæ¸…ç†ä»£ç  ======================
        # åœ¨ç¨‹åºå³å°†ç»“æŸæ—¶ï¼Œæ‰‹åŠ¨è§¦å‘èµ„æºé‡Šæ”¾ï¼Œé¿å…é€€å‡ºæ—¶å¡æ­»

        print("\nè®­ç»ƒå…¨éƒ¨å®Œæˆï¼Œæ­£åœ¨æ‰‹åŠ¨æ¸…ç†å†…å­˜...")

        # æ£€æŸ¥coachå¯¹è±¡æ˜¯å¦å­˜åœ¨ï¼Œå¹¶æ¸…ç©ºå…¶å†…éƒ¨çš„å¤§æ•°æ®ç»“æ„
        if 'coach' in locals() and hasattr(coach, 'training_data'):
            coach.training_data.clear()
            print("ç»éªŒå›æ”¾æ± å·²æ¸…ç©ºã€‚")

        # æ‰‹åŠ¨è°ƒç”¨Pythonçš„åƒåœ¾å›æ”¶å™¨
        import gc

        gc.collect()

        print("å†…å­˜æ¸…ç†å®Œæˆã€‚ç¨‹åºå³å°†é€€å‡ºã€‚")
        # ===============================================================